{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Machine-learning-practice\" data-toc-modified-id=\"Machine-learning-practice-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Machine learning practice</a></div><div class=\"lev2 toc-item\"><a href=\"#Packages\" data-toc-modified-id=\"Packages-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Packages</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load data</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Keras-and-run-first-network\" data-toc-modified-id=\"Load-Keras-and-run-first-network-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Load Keras and run first network</a></div><div class=\"lev2 toc-item\"><a href=\"#Network-Metrics\" data-toc-modified-id=\"Network-Metrics-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Network Metrics</a></div><div class=\"lev1 toc-item\"><a href=\"#Physics-Example\" data-toc-modified-id=\"Physics-Example-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Physics Example</a></div><div class=\"lev2 toc-item\"><a href=\"#Another-helpful-python-package\" data-toc-modified-id=\"Another-helpful-python-package-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Another helpful python package</a></div><div class=\"lev2 toc-item\"><a href=\"#Build-the-combined-dataset\" data-toc-modified-id=\"Build-the-combined-dataset-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Build the combined dataset</a></div><div class=\"lev2 toc-item\"><a href=\"#Split-the-data-into-a-test-a-testing-and-training-set.\" data-toc-modified-id=\"Split-the-data-into-a-test-a-testing-and-training-set.-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Split the data into a test a testing and training set.</a></div><div class=\"lev2 toc-item\"><a href=\"#Build-and-train-a-network.\" data-toc-modified-id=\"Build-and-train-a-network.-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Build and train a network.</a></div><div class=\"lev1 toc-item\"><a href=\"#Normalizing-the-data\" data-toc-modified-id=\"Normalizing-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Normalizing the data</a></div><div class=\"lev2 toc-item\"><a href=\"#Build-and-fit-a-new-model\" data-toc-modified-id=\"Build-and-fit-a-new-model-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Build and fit a new model</a></div><div class=\"lev2 toc-item\"><a href=\"#Explore-training-methods\" data-toc-modified-id=\"Explore-training-methods-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Explore training methods</a></div><div class=\"lev2 toc-item\"><a href=\"#Network-architecture\" data-toc-modified-id=\"Network-architecture-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Network architecture</a></div><div class=\"lev2 toc-item\"><a href=\"#Longer-training\" data-toc-modified-id=\"Longer-training-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Longer training</a></div><div class=\"lev2 toc-item\"><a href=\"#Regularize-the-network\" data-toc-modified-id=\"Regularize-the-network-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Regularize the network</a></div><div class=\"lev1 toc-item\"><a href=\"#Different-representation-of-the-data\" data-toc-modified-id=\"Different-representation-of-the-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Different representation of the data</a></div><div class=\"lev2 toc-item\"><a href=\"#Neural-networks\" data-toc-modified-id=\"Neural-networks-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Neural networks</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning practice\n",
    "\n",
    "Welcome to the second tutorial. In the first session, we started with linear regression and built up to logistic regression. We saw that this form of machine learning is really just an optimization problem. We define a test metric to determine how good of a fit we have, and then we use gradient descent to find the minimum of the loss function.\n",
    "\n",
    "From this, we were able to build up to program a neural network from scratch! The hardest part of this was the back propagation, getting the gradients further back gets hard to keep track of everything. This is where some of the power of modern tools comes into play. Packages like [tensorflow](https://www.tensorflow.org/) and [pytorch](https://pytorch.org/) have an method to keep track of the gradients along the way, even for layers much more complicated that what we have used so far.\n",
    "\n",
    "The docker container we are using contains *tensorflow* and [keras](https://keras.io/), which is built on top of tensorflow (or some other systems) and makes building networks very easy. In the rest of this tutorial we will explore how to make simple networks in *Keras*. As this will be easier than before, we will have time to gain an understanding for:\n",
    "- Optimizers other than gradient descent\n",
    "- Adding depth instead of width\n",
    "- Different activation functions\n",
    "\n",
    "In addition, we will look at some of the *important/best* practices for machine learning. These include:\n",
    "- Metrics for comparing results\n",
    "- Train/Validate/Test sets\n",
    "- Preprocessing\n",
    "\n",
    "This is the first time I have used this tutorial. Please send bugs or typos to bostdiek@gmail.com\n",
    "\n",
    "## Packages\n",
    "First, lets import the packages we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Allows for plotting within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Packages from last time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For organization later\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The commands make the plots look nice\n",
    "plt.rcParams.update({'font.family': 'cmr10',\n",
    "                     'font.size': 12,\n",
    "                     'axes.unicode_minus': False,\n",
    "                     'axes.labelsize': 12,\n",
    "                     'figure.figsize': (3, 3),\n",
    "                     'figure.dpi': 80,\n",
    "                     'mathtext.fontset': 'cm',\n",
    "                     'mathtext.rm': 'serif',\n",
    "                     'xtick.direction': 'in',\n",
    "                     'ytick.direction': 'in',\n",
    "                     'xtick.top': True,\n",
    "                     'ytick.right': True\n",
    "                     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "As a very brief introduction to keras, we will use the same 2D data from last session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training = np.load('../data/tutorial_1_data/logistic_regression_training.npy')\n",
    "toy_X_Train = training[:, :2]\n",
    "toy_Y_Train = training[:, 2].reshape(-1, 1)  # makes it a vector\n",
    "\n",
    "validation = np.load('../data/tutorial_1_data/logistic_regression_training.npy')\n",
    "toy_X_Val = validation[:, :2]\n",
    "toy_Y_Val = validation[:, 2].reshape(-1, 1)  # makes it a vector\n",
    "\n",
    "testing = np.load('../data/tutorial_1_data/logistic_regression_testing.npy')\n",
    "toy_X_Test = testing[:, :2]\n",
    "toy_Y_Test = testing[:, 2].reshape(-1, 1)  # makes it a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Keras and run first network\n",
    "Load the parts of the package we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras import Sequential    # This is the easiest method to build\n",
    "                                # a network, but not the most flexible\n",
    "    \n",
    "from keras.layers import Dense  # For layers which are fully connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is not necessary, but will supress some of the warning messages from tensorflow about depreciated components, which we are not using anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_0 = Sequential()\n",
    "\n",
    "# First hidden\n",
    "model_0.add(Dense(units=10,          # How many nodes\n",
    "                  activation='relu', # What activation to use\n",
    "                  input_dim=2        # Only needed for the first layer and\n",
    "                                     # sets the dimensionality of the data\n",
    "                 )\n",
    "           )\n",
    "\n",
    "# Output\n",
    "model_0.add(Dense(units=1,              # Only one answer\n",
    "                  activation='sigmoid'  # Map between 0 and 1\n",
    "                 )\n",
    "           )\n",
    "\n",
    "# Now compile the network and tell it\n",
    "# what loss function and optimizer to use\n",
    "model_0.compile(loss='binary_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This serves as your example on building a network. You will have to generalize from this in the rest of the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is stochastic gradient descent. Part of the 'stochastic' part is that the gradients are only computed on a small fraction of the dataset, instead of the whole dataset. This is the `batch_size` parameter that we will see below. See the website for the avilable [optimizers](https://keras.io/optimizers/) and [loss functions](https://keras.io/losses/).\n",
    "\n",
    "We can also print a summary of our network to make sure that it looks like what we wanted, and to count the free parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "history_0 = model_0.fit(x=toy_X_Train,  # The input data\n",
    "                        y=toy_Y_Train,  # The labels we want to predict\n",
    "                        epochs=500,     # How long to train\n",
    "                        batch_size=32,  # How many events to use to compute the gradient\n",
    "                        validation_data=(toy_X_Val, toy_Y_Val), # the data and labels of the validation set\n",
    "                        verbose=True    # To print out how the training is going\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all it took, much easier than before! Now we plot the losses on the validation and training sets like we did before, to make sure that we are not over-fitting the data. These are returned as when calling `model.fit()` which is why we set it equal to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_0.history['loss'], label='Training')\n",
    "plt.plot(history_0.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the losses are similar, so we are not overfitting. Notice that there is more noise that last tutorial, this is because of the batches. The gradient can sometimes move us in the wrong direction because we are only taking a subset of the data each time. However, each *epoch*, or iteration through the data, now updates the paramters more times. (The length of the dataset divided by the batch size). With this, it takes less iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Metrics\n",
    "The loss is clearly one way to determine which model is better. However, in physics we want to know what the efficiency of a cut is, how much background is cut out, etc. We will use the **Test** data for this. The *Validation* data could be used for this, because it was not explicitly trained on. However, we often use the validation data as a way to stop training early, in which case the network is not entirely independent from it. Metrics comparing results should **always** be on same independent data if possible.\n",
    "\n",
    "The first metric we will look at is the Receiver Operating Characteristic (ROC) curve. To get this, we first need to get the predictions for the network on our test data.\n",
    "\n",
    "**Exercise**: Use look up how to use the `predict` function of our model. Make a get a prediction for each event in the test dataset. (Hint, use model_0.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "preds_model_0 = model_0.predict(\n",
    "### End code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot these results, first splitting into the signal and background cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "TestSigs = preds_model_0[toy_Y_Test == 1]\n",
    "TestBack = preds_model_0[toy_Y_Test == 0]\n",
    "\n",
    "plt.hist(TestBack, histtype='step',  # density='True',\n",
    "         bins=20, range=(0, 1), label='Background')\n",
    "plt.hist(TestSigs, histtype='step',  # density='True',\n",
    "         bins=20, range=(0, 1), label='Signal')\n",
    "plt.xlabel('Network output')\n",
    "plt.ylabel('Events/bin')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 1)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.legend(loc='upper center', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the background events are clustered around 0 and the signal events are clustered around 1. Imagine cutting the data, and only the datapoints with a score larger than a certin number. If we start the cut at 0, all of the events make it through, so we have a signal efficiency $\\epsilon_S$ and a background efficiency $\\epsilon_B$ equal to 1. In the opposite extreme, we could only take events with a score larger than 1, in which case no events pass, thus $\\epsilon_S = \\epsilon_B = 0$. As we scan the value of the cut, $\\epsilon_S$ and $\\epsilon_B$ will work from 0 to 1, plotting these parametrically defines the ROC Curve. If $\\epsilon_S=\\epsilon_B$ everywhere, our model is never gaining an advantage over random guessing.\n",
    "\n",
    "To compute the ROC curve, we will use part of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the ROC curve. (Hint, to bring up the doc string, run a cell with a question mark, as shown below). We should only need to supply y_true and y_score. The other arguments we can leave as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "roc_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "fpr_model_0, tpr_model_0, thresholds_model_0 = roc_curve(\n",
    "### End code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make the plot. The fpr is the *False positive rate* which is the same thing as $\\epsilon_B$. The tpr is *True positie rate* which is $\\epsilon_S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Plot the epsilon_S along the y axis and epsilon_B along the x\n",
    "plt.plot(fpr_model_0, tpr_model_0)\n",
    "plt.plot([0,1], [0, 1], ls=':', color='k')  # interpolating between all events and no events\n",
    "plt.xlabel(r'$\\epsilon_B$')\n",
    "plt.ylabel(r'$\\epsilon_S$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect classifier would be able to get all of the signal events $\\epsilon_S=1$ and none of the background $\\epsilon_B=0$. Curves that are much above the dotted line are good classifier. Another metric related to these is the **Area under the curve** or (AUC). An AUC of 1 is perfect, while an AUC of 0.5 is random guessing. The AUC can be computed easily within `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the area under the curve using `auc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "auc_model_0 = auc(\n",
    "### End code here ###\n",
    "\n",
    "print('The area under the ROC curve is {0:.3f}'.format(auc_model_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is a good metric, because it scans accross the efficiencies. However, it is dominated by the values at large $\\epsilon_B$. In cutting out SM background at the LHC, we often need to cut out lots of background. The other metric we will use is the amount of background rejection at a signal efficiency of 0.7. There are a few ways that we can find this, but one of the easiest is to make an interpolating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make an interpolating function to be able to compute the false positive rate for any true positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "?interp1d\n",
    "# Hint: We should only need to supply x and y, leave the rest of the arguments as their default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "background_eff = interp1d(\n",
    "### End code here ###\n",
    "\n",
    "# Now we use the function to get the background efficiency at a\n",
    "# fixed signal efficiency of 0.7\n",
    "background_at_sig_70 = background_eff(0.7)\n",
    "print('The e_B at a fixed e_S=0.7' + ': {0:.3e}'.format(background_at_sig_70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a small number, we can invert it. We can also plot the ROC curve with the signal efficiency on the $x$ axis, and invert the background efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(tpr_model_0,\n",
    "         1/fpr_model_0, # in python 3 we can easily do division like this\n",
    "         label='{0:.1f}'.format(1/background_at_sig_70)\n",
    "        )\n",
    "plt.xlabel('$\\epsilon_{S}$')\n",
    "plt.ylabel('$1/\\epsilon_{B}$')\n",
    "plt.xlim(0,1)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This toy dataset does not have very large test set, so we cannot get great metrics. When you design a study, this needs to be kept in mind. Your test set needs to be large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the basics on how to use `Keras` and how to compute metrics on our models, let's move to a physics example. In the `tutorial_2_data` are events generated with Sherpa which are di-gluon and di-quark (parton-level) events from 13 TeV proton-proton collisions. We then analyzed the resulting files by clustering the final-state particles with the anti-kt alogithm. The consituents which have been clustered into the jet with the largest transverse momentum are then put into a comma seperated file. Each row of the file is the hardest jet in each event. The columns are the $p_T$, $\\eta$, and $\\phi$ of up to 10 consituents. Note that we are treating the particles as massless. If there are less than 10 constituents, we fill its place with 0. Finally, the last column is a 0 for the gluons and a 1 for the quarks. Our goal is to make a classifier to distinguish the quarks from the gluons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We only have around 25000 samples for each jet type. For deep learning, it would be better to have much larger samples. While the results we get below may not look like deep learning is buying us anything, I hope that you at least get something from the concepts that we are looking at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another helpful python package\n",
    "To load the csv files, I will use the pandas package. I find it very helpful, but don't make the most use of it in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This cell just gets the names for the columns of the csv files\n",
    "mynames = []\n",
    "for i in range(10):\n",
    "    mynames.append('pt_' + str(i))\n",
    "    mynames.append('eta_' + str(i))\n",
    "    mynames.append('phi_' + str(i))\n",
    "mynames.append('label')\n",
    "print(mynames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "gluons = pd.read_csv('../data/tutorial_2_data/gluons.csv',\n",
    "                     names=mynames\n",
    "                    )\n",
    "quarks = pd.read_csv('../data/tutorial_2_data/quarks.csv',\n",
    "                     names=mynames\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of pandas is the .describe() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "quarks.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "gluons.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the dataset. Plot the distributions of the $p_T$, $\\eta$, and $\\phi$ for the 10 different consituents of the quark and gluon jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "for name in mynames:\n",
    "    if 'pt' in name:\n",
    "        plt.subplot(1, 3, 1)\n",
    "        h, bins, _ = plt.hist(gluons[name], histtype='step', bins=100)\n",
    "        plt.hist(quarks[name], histtype='step', bins=bins)\n",
    "        plt.plot([], [], color='C0', label='Gluon')\n",
    "        plt.plot([], [], color='C1', label='Quark')\n",
    "        plt.xlabel('$p_T$ [GeV]')\n",
    "        plt.ylabel('Arb.')\n",
    "        plt.title('Consituent {0}'.format(name.split('_')[-1]))\n",
    "        plt.yscale('log')\n",
    "        plt.legend(loc='best', frameon=False, fontsize=10)\n",
    "        \n",
    "    if 'eta' in name:\n",
    "        plt.subplot(1, 3, 2)\n",
    "        h, bins, _ = plt.hist(gluons[name], histtype='step', bins=100, range=(-2, 2))\n",
    "        plt.hist(quarks[name], histtype='step', bins=bins)\n",
    "        plt.plot([], [], color='C0', label='Gluon')\n",
    "        plt.plot([], [], color='C1', label='Quark')\n",
    "        plt.xlabel('$\\eta$')\n",
    "        plt.ylabel('Arb.')\n",
    "        plt.title('Consituent {0}'.format(name.split('_')[-1]))\n",
    "        plt.yscale('log')\n",
    "        \n",
    "    if 'phi' in name:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        h, bins, _ = plt.hist(gluons[name], histtype='step', bins=100, range=(-np.pi, np.pi))\n",
    "        plt.hist(quarks[name], histtype='step', bins=bins)\n",
    "        plt.plot([], [], color='C0', label='Gluon')\n",
    "        plt.plot([], [], color='C1', label='Quark')\n",
    "        plt.xlabel('$\\phi$')\n",
    "        plt.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi],\n",
    "                   ['$-\\pi$', '$-\\pi/2$', '0', '$\\pi/2$', '$\\pi$']\n",
    "                  )\n",
    "        plt.ylabel('Arb.')\n",
    "        plt.title('Consituent {0}'.format(name.split('_')[-1]))\n",
    "        plt.yscale('log')\n",
    "        plt.minorticks_on()\n",
    "        \n",
    "        plt.tight_layout(w_pad=2)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(9, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are definitely differences in some of these distributions. Noticeably, the quark jets tend to have fewer constituents, so there are more events in the `0` bin, because of the way we zero padded for empty events. Now let's put the two sets together into a single dataset so that we can feed it into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the combined dataset\n",
    "Our data is currently in two different pandas DataFrame objects. To run the events through a neural network for training, we need to combined them. \n",
    "Use `np.vstack()` to combine the quark and gluon arrays into a single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.vstack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "Combined = np.vstack(\n",
    "### End code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print the shape to see that it makes sense. Do we have the same number of columns as we had before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have the second dimension of the shape be 31. This is split between the 30 inputs and the 1 label. Now we  strip off the label and the data into seperate arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_data = Combined[:, :-1]\n",
    "Y_data = Combined[:, -1].reshape(-1, 1)\n",
    "\n",
    "print(X_data.shape, Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into a test a testing and training set.\n",
    "Now all of our data is in one array. This would work, but we want an independent test set, which we do not train on. To do this in a very easy way, we will use part of the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Split the data into a testing a training set. Make the test set have 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "### End code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the data (marked by X) has the same number of columns. Make sure both X_train and y_train have the same number of rows, and similarly for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a network.\n",
    "Make a network which has 1 hidden layer which uses either a `relu` or `tanh` activation. As our data is 30 dimensional, let's first try using 50 nodes on the hidden layer. Then have an output layer with 1 node, using a sigmoid activation. Compile the network using the binary cross entropy loss function and the stochastic gradient descent optimizer.\n",
    "\n",
    "**Exercise:** Finish building and compiling the model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(\n",
    "model_1.add(\n",
    "model_1.compile(\n",
    "### End code here ###\n",
    "    \n",
    "# Print a summary of your model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model built. Let's fit it.\n",
    "\n",
    "**Exercise:** Use model_1.fit to train the model. Train for 10 epochs and use validation split of 10%. Note that we haven't used this option before, so I have include that here, but will not in the future.\n",
    "\n",
    "Hint: This model should not train well. Do not be afraid if it doesn't work. This is part of the process.\n",
    "\n",
    "*I have been told that this actually does train well for some people, and that it depends on your system and which version of python, keras, and tensorflow you are using. It is still a **very good idea** to normalize the data as we will do below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "model_1.fit(\n",
    "### End code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the loss did not seem to decrease as we were training. This doesn't seem good! To make sure, lets see how the test set does.\n",
    "\n",
    "**Exercise:**\n",
    "Compute the predictions and then tthe ROC curve and AUC for model_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "preds_model_1 = model_1.predict(\n",
    "fpr_model_1, tpr_model_1, thresholds_model_1 = roc_curve(\n",
    "auc_model_1 = auc(\n",
    "### End code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr_model_1, tpr_model_1, label='qvq model 1')\n",
    "plt.xlabel('$\\epsilon_B$')\n",
    "plt.ylabel('$\\epsilon_S$')\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.show()\n",
    "\n",
    "print('The AUC is {0:0.2f}'.format(auc_model_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms what we thought! The network does not seem to be able to figure anything out about this dataset. Part of this has to do with the different scales of our data, let's explore this more in the next section.\n",
    "\n",
    "# Normalizing the data\n",
    "If we go back and look at the distributions of the input data, we notice that the ranges of the values are very different, especially the transvere momenta. Trying to find the minimum of the loss is much harder when the scales and the variance of the data is very different. To account for this, we normalize the data. There are many ways to do this, but one common way is to individually subtract the mean of each obsesrvable and then divide by the variance. Thus each input variable will have 0 mean and unit variance. Another common method is to scale the data such that the min is at 0 and the max is at 1. \n",
    "\n",
    "Both of these methods distort the data, and break Lorentz invariance. There is some research into making networks (and scaling) which respect physics invariant, but we can get good classifiers without worrying about this. \n",
    "\n",
    "For now, let use the 0 mean and unit variance. We can compute this ourselves, or use the `sklearn.preprocessing.StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "StandardScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SS = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and transform the data the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "X_train_scaled = \n",
    "### Ene code here ###\n",
    "\n",
    "# Now ONLY TRANSFORM the test data\n",
    "### Enter code here ##\n",
    "X_test_scaled = \n",
    "### End code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and fit a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all of the different random number generators to the same values is actually very hard within Keras/Tensorflow/Numpy. Because of this, exact reproducibility is not possible. To **really** trust our results, the best thing would be to retrain many times and compute means and variances of our results. For time we will not do that here. \n",
    "\n",
    "**NOTE:** Your results may differ from what the discussion around it says because of this. I have tried to write in in a way that you can generalize, but please let me know if something does not make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build an identical network to our previous one. We give it a different name so we can compare with the old results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "model_2 = Sequential()\n",
    "model_2.add(\n",
    "model_2.add(\n",
    "model_2.compile(\n",
    "### End code here ###\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now fit the network using the scaled data which we just computed. Use 15 epochs for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(X_train_scaled, y_train,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=15\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['loss'], label='Training')\n",
    "plt.plot(history_2.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.title('SGD - normalized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note from this.\n",
    "\n",
    "1. The training loss has not stopped going down, so we should really train for more than 15 epochs. However, right now we are just testing things out, and this gives us an idea for how the model is doing.\n",
    "2. This is the first time we have seen the training and validation loss being different. This shows that we are slightly over fitting the data. This is not necessarily a big deal at this point, but something to look out for. If we trained for longer and the training loss got further and further from the validation loss, we would want to do something to fix this. Methods to do this are called *regularization*, and can include adding extra terms to the loss so that individual network weights cannot get to big ([l1 or l2](https://keras.io/regularizers/) regularization) or randomly ignoring certain nodes during training ([dropout](https://keras.io/layers/core/#dropout)). \n",
    "\n",
    "  Another way to compete against over-fitting is to get more data. You can test this by instead fitting on a very small amount of data. You should see the losses diverge then.\n",
    "\n",
    "Even though we are slightly over-fitting, and the training has not reached its asymptote, lets compute the test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "preds_model_2 = model_2.predict(X_test_scaled)\n",
    "fpr_model_2, tpr_model_2, thresholds_model_2 = roc_curve(y_true=y_test, y_score=preds_model_2)\n",
    "auc_model_2 = auc(fpr_model_2, tpr_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "background_eff_2 = interp1d(tpr_model_2, fpr_model_2)\n",
    "background_at_sig_50_2 = background_eff_2(0.5)\n",
    "print('The $\\epsilon_B$ at a fixed $\\epsilon_S=0.5$' + ': {0:.3e}'.format(background_at_sig_50_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(preds_model_2[y_test == 0], range=(0, 1), bins=50, histtype='step', color='C2')\n",
    "plt.hist(preds_model_2[y_test == 1], range=(0, 1), bins=50, histtype='step', color='C3')\n",
    "plt.xlim(0, 1)\n",
    "plt.plot([], [], color='C2', label='gluon')\n",
    "plt.plot([], [], color='C3', label='quark')\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.xlabel('Network output')\n",
    "plt.ylabel('Jets / bin')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_model_1, tpr_model_1, label='model 1')\n",
    "plt.plot(fpr_model_2, tpr_model_2, label='(scaled data)')\n",
    "plt.xlabel('$\\epsilon_B$')\n",
    "plt.ylabel('$\\epsilon_S$')\n",
    "plt.legend(loc=('best'), frameon=False)\n",
    "\n",
    "plt.tight_layout(w_pad=2)\n",
    "plt.show()\n",
    "\n",
    "print('The AUC is {0:0.4f}'.format(auc_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(tpr_model_1, 1/fpr_model_1,  label='qvq model 1')\n",
    "plt.plot(tpr_model_2, 1/fpr_model_2, label='qvq model 2 (scaled data)')\n",
    "plt.ylabel('$1/\\epsilon_B$')\n",
    "plt.xlabel('$\\epsilon_S$')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 1)\n",
    "plt.legend(loc=(1.05,0.2), frameon=False)\n",
    "plt.show()\n",
    "\n",
    "print('The 1 / epsilon_B at a fixed e_S=0.5' + ': {0:.2f}'.format(1/background_at_sig_50_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we see that we are getting a predictive model. Normalizing the data made a huge difference! Without normalizing, the network couldn't even start to train. The normalized model does better on the training data, and it also generalizes to the test data. We are able to split the quark and gluon jets. We will now cycle through some different options to see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore training methods\n",
    "Stochastic gradient descent is what we worked out last time, where each update to the model parameters is in the direction of the gradient at that point. However, there are other methods, which keep 'momentum' as the optimizer rolls down the hill of the loss potential. We can use these easily because keras has the optimizers built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Add a network to the dictionary for each training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainining_methods=['rmsprop', 'adam']\n",
    "\n",
    "# Make python dictionaries to save our results\n",
    "training_method_models = {}\n",
    "training_histories = {}\n",
    "\n",
    "for method in trainining_methods:\n",
    "    print('Using ' + method)\n",
    "    \n",
    "    ### Enter code here ###\n",
    "    temp_model = \n",
    "    temp_model.add(\n",
    "    temp_model.add(\n",
    "    temp_model.compile(\n",
    "    \n",
    "    # Fit the model\n",
    "    temp_history = temp_model.fit(\n",
    "    ### End code here ###\n",
    "    \n",
    "    # Now put the model into the dictionary\n",
    "    training_method_models[method] = temp_model\n",
    "    \n",
    "    # Now train the model\n",
    "    training_histories[method] = temp_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the comparisons of loss of the training methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['loss'], color='C0', label='SGD')\n",
    "plt.plot(training_histories['rmsprop'].history['loss'], color='C1', label='RMSprop')\n",
    "plt.plot(training_histories['adam'].history['loss'], color='C2', label='Adam')\n",
    "\n",
    "plt.plot(training_histories['rmsprop'].history['val_loss'], color='C1', ls=':')\n",
    "plt.plot(training_histories['adam'].history['val_loss'], color='C2', ls=':')\n",
    "plt.plot(history_2.history['val_loss'], color='C0', ls=':')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "# plt.title('SGD - normalized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on the training data, Adam and RMSPROP do better than SGD. In general, SGD will eventually converge to the minimum, but it is slower than other alogrithms. The most popular optimizer to use currently is Adam. It is also important to note that is the methods doing better on the training sample do not always do better on the validation data. Again, all of the validation losses are still improving, so training is still making the model do better on data it hasn't seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compute our test metrics. We will again add these to dictionaries so that they are easy to call later.\n",
    "\n",
    "**Exercise:** Compute the test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "preds_methods = {}\n",
    "fpr_methods = {}\n",
    "tpr_methods = {}\n",
    "thresholds_methods = {}\n",
    "auc_methods = {}\n",
    "working_point_methods = {}\n",
    "\n",
    "for method in trainining_methods:\n",
    "    \n",
    "    # Get the trained model from our models dictionary\n",
    "    tmp_model = training_method_models[method]\n",
    "    \n",
    "    ### Enter code here ###\n",
    "    tmp_preds = tmp_model.predict(\n",
    "    tmp_fpr, tmp_tpr, tmp_thresholds = roc_curve(\n",
    "    tmp_auc = auc(\n",
    "    ### End code here ###\n",
    "    \n",
    "    # Fill in the different result dictionaries\n",
    "    preds_methods[method] = tmp_preds\n",
    "    fpr_methods[method] = tmp_fpr\n",
    "    tpr_methods[method] = tmp_tpr\n",
    "    thresholds_methods[method] = tmp_thresholds\n",
    "    auc_methods[method] = tmp_auc\n",
    "    \n",
    "    # Now compute the working point at a signal efficiency of 0.5\n",
    "    ### Enter code here ###\n",
    "    tmp_background_eff = interp1d(\n",
    "    tmp_working_point = tmp_background_eff(\n",
    "    ### End code here \n",
    "    \n",
    "    # Fill in the dictionary\n",
    "    working_point_methods[method] = tmp_working_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(tpr_model_2, 1/fpr_model_2, label='SGD')\n",
    "plt.plot(tpr_methods['rmsprop'], 1/fpr_methods['rmsprop'], label='rmsprop')\n",
    "plt.plot(tpr_methods['adam'], 1/fpr_methods['adam'], label='adam')\n",
    "plt.ylabel('$1/\\epsilon_B$')\n",
    "plt.xlabel('$\\epsilon_S$')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(1, 50)\n",
    "plt.legend(loc=(1.05,0.3), frameon=False)\n",
    "plt.show()\n",
    "\n",
    "print('SGD: 1 / epsilon_B at a fixed e_S=0.5' + ': {0:.2f}'.format(1/background_at_sig_50_2))\n",
    "print('SGD: AUC={0:.4f}'.format(auc_model_2))\n",
    "print('')\n",
    "print('RMSProp: 1 / epsilon_B at a fixed e_S=0.5' + ': {0:.2f}'.format(1/working_point_methods['rmsprop']))\n",
    "print('RMSProp: AUC={0:.4f}'.format(auc_methods['rmsprop']))\n",
    "print('')\n",
    "print('Adam: 1 / epsilon_B at a fixed e_S=0.5' + ': {0:.2f}'.format(1/working_point_methods['adam']))\n",
    "print('Adam: AUC={0:.4f}'.format(auc_methods['adam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, it doesn't appear that any of the optimizers works particularly better on the test set. The best one at the working point is Adam. Up to now, we have left the architecture of the network fixed, but have changed the data (normalized it) and changed the optimizers. Now let's see if we can get a better score by either adding width to the hidden layer or by making the network deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "Now we explore the differnt network shapes. I will put this into a dictionary which we can scan over easily. For now, just build and compile the network, but don't train it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "archs = OrderedDict()\n",
    "\n",
    "# Smaller nodes per layer, but more layers\n",
    "\n",
    "# The name of the model tells me how many nodes per dense layer\n",
    "# 1 hidden layer\n",
    "tmp_model = Sequential()\n",
    "tmp_model.add(Dense(50, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "tmp_model.add(Dense(1, activation='sigmoid'))\n",
    "tmp_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "archs['D50D1'] = tmp_model\n",
    "\n",
    "# 2 hidden layers\n",
    "tmp_model = Sequential()\n",
    "tmp_model.add(Dense(50, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "tmp_model.add(Dense(50, activation='relu'))\n",
    "tmp_model.add(Dense(1, activation='sigmoid'))\n",
    "tmp_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "archs['D50D50D1'] = tmp_model\n",
    "\n",
    "# 3 hidden layers\n",
    "## Follow the same pattern\n",
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "archs['D50D50D50D1'] = tmp_model\n",
    "### Enter code here ###\n",
    "\n",
    "# 4 hidden layers\n",
    "## Follow the same pattern\n",
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "tmp_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "archs['D50D50D50D50D1'] = tmp_model\n",
    "### Enter code here ###\n",
    "\n",
    "# Now we change the style, instead of going deeper, lets test a single layer with differnt number of nodes.\n",
    "\n",
    "# 1 hidden layer - 10 Nodes\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "tmp_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "archs['D10D1'] = tmp_model\n",
    "\n",
    "# 1 hidden layer - 20 Nodes\n",
    "## Follow the same pattern\n",
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "tmp_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "archs['D20D1'] = tmp_model\n",
    "### Enter code here ###\n",
    "\n",
    "# 1 hidden layer - 100 Nodes\n",
    "## Follow the same pattern\n",
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "archs['D100D1'] = tmp_model\n",
    "### Enter code here ###\n",
    "\n",
    "# 1 hidden layer - 500 Nodes\n",
    "## Follow the same pattern\n",
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "archs['D500D1'] = tmp_model\n",
    "### Enter code here ###\n",
    "\n",
    "# 1 hidden layer - 1000 Nodes\n",
    "## Follow the same pattern\n",
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "archs['D1000D1'] = tmp_model\n",
    "### Enter code here ###\n",
    "\n",
    "# 1 hidden layer - 10000 Nodes\n",
    "## Follow the same pattern\n",
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "#Fill in\n",
    "archs['D10000D1'] = tmp_model\n",
    "### Enter code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scan over the models in our dictionary and train them is a single loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for model in archs:\n",
    "    archs[model].summary()\n",
    "    training_histories[model] = archs[model].fit(X_train_scaled, y_train,\n",
    "                                                  validation_split=0.1,\n",
    "                                                  epochs=15,\n",
    "                                                  verbose=2\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the losses. Because we have so many models we are comparing, I am going to put the training and validation data on different panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "for i, model in enumerate(archs):\n",
    "    if i > 3:\n",
    "        myls = '--'\n",
    "    else:\n",
    "        myls = '-'\n",
    "    color = 'C{0}'.format(i)\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_histories[model].history['loss'], color=color, ls=myls)\n",
    "    plt.title('Training')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    ax2 = plt.subplot(1, 2, 2, sharey=ax1)\n",
    "    plt.plot(training_histories[model].history['val_loss'], color=color, label=model, ls=myls)\n",
    "    plt.legend(loc=(1.05, 0.1))\n",
    "    plt.title('Validation')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shallow but wide networks are shown with dashed lines. The models which do best on the *training* data are the deep networks. Again, all of the networks are having a problem genralizing, which could be fixed with more data or attempts to regularlize.\n",
    "\n",
    "Now compute the test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "preds_archs = {}\n",
    "fpr_archs = {}\n",
    "tpr_archs = {}\n",
    "thresholds_archs = {}\n",
    "auc_archs = {}\n",
    "working_point_archs = {}\n",
    "\n",
    "for model in archs:\n",
    "    preds_archs[model] = archs[model].predict(X_test_scaled)\n",
    "    fpr_archs[model], tpr_archs[model], thresholds_archs[model] = roc_curve(y_true=y_test,\n",
    "                                                                            y_score=preds_archs[model])\n",
    "    auc_archs[model] = auc(fpr_archs[model], tpr_archs[model])\n",
    "    \n",
    "    background_eff = interp1d(tpr_archs[model], fpr_archs[model])\n",
    "    working_point_archs[model] = background_eff(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(archs):\n",
    "    if i > 3:\n",
    "        myls = '--'\n",
    "    else:\n",
    "        myls = '-'\n",
    "    plt.plot(tpr_archs[model], 1/fpr_archs[model], label=model, ls=myls)\n",
    "    print(model + ': 1/ e_B at a fixed e_S=0.5' + ': {0:.3f}'.format(1/working_point_archs[model])),\n",
    "    print('\\t\\tAUC={0:.3f}'.format(auc_archs[model]))\n",
    "plt.ylabel('$1/\\epsilon_B$')\n",
    "plt.xlabel('$\\epsilon_S$')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 1)\n",
    "plt.legend(loc=(1.05,0.), frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is still not much difference in the results. This is, again, in part due to the size of our samples. \n",
    "Currently, these metrics would suggest using the model which is wider, rather than deeper. However, we should really run things many times and average over our results. I wouldn't actually think these difference are statistically meaningfull at this point. If you results are different than mine, it wouldn't surprise me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer training\n",
    "As we looked at the loss histories, we see that the models are sill improving. This implies that we should not have stopped training. How do we know how many epochs to use?\n",
    "\n",
    "One way to do the is to watch the validation loss. If it has stopped improving, we can lower the learning rate so that smaller steps are taken. If we do this a couple of times, we should get a very descent classifier.\n",
    "Look up `keras.callbacks.EarlyStopping` and `keras.callbacks.ReduceLROnPlateau`. We are also going to explicity load the Adam optimizer and tell it the learning rate to start at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tf.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "EarlyStopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ReduceLROnPlateau?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the patience of the Early stopping to 15, restore_best_weights to true\n",
    "\n",
    "Set the patience of the ReduceLROnPlateau to 5, and cut the learning rate in half, with a minimum learning rate of 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=,                # If the validation loss has not improved for 15 epochs, stop training\n",
    "                   restore_best_weights=,  # When we stop, go back to when it generalized to the validation data the best\n",
    "                   verbose=)\n",
    "\n",
    "reduce_rate = ReduceLROnPlateau(patience=, # If the validation loss has not improved for 5 epochs, lower the rate\n",
    "                                factor=, # cut the rate in half\n",
    "                                verbose=,  # Tell us when this happens\n",
    "                                min_lr= # If the learning rate is too small, the updates don't do very much\n",
    "                               )\n",
    "my_callbacks = [es, reduce_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your favorite model from above. Add that to the dictionary with a name of 'Final'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Enter code here ###\n",
    "tmp_model = Sequential()\n",
    "\n",
    "### End code here ###\n",
    "\n",
    "# Now we compile, where we tell Adam where to start at.\n",
    "tmp_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-3))\n",
    "archs['EarlyStopping'] = tmp_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_histories['EarlyStopping'] = archs['EarlyStopping'].fit(X_train, y_train,\n",
    "                                                                 validation_split=0.1,\n",
    "                                                                 epochs=200,\n",
    "                                                                 callbacks=my_callbacks,\n",
    "                                                                 verbose=1\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(training_histories['EarlyStopping'].history['loss'], color='C0', label='Final')\n",
    "plt.plot(training_histories['EarlyStopping'].history['val_loss'], color='C0', ls=':')\n",
    "plt.legend(loc=(1.05, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for model in ['EarlyStopping']:\n",
    "    preds_archs[model] = archs[model].predict(X_test)\n",
    "    fpr_archs[model], tpr_archs[model], thresholds_archs[model] = roc_curve(y_true=y_test,\n",
    "                                                                            y_score=preds_archs[model])\n",
    "    auc_archs[model] = auc(fpr_archs[model], tpr_archs[model])\n",
    "    \n",
    "    background_eff = interp1d(tpr_archs[model], fpr_archs[model])\n",
    "    working_point_archs[model] = background_eff(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the the model with a short train time. Add the string name of your model to the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "my_compare = ['',  # put your model comparison name in here\n",
    "              'EarlyStopping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for model in my_compare:\n",
    "    plt.plot(tpr_archs[model], 1/fpr_archs[model], label=model)\n",
    "    print(model + ': 1/ e_B at a fixed e_S=0.5' + ': {0:.2f}'.format(1 / working_point_archs[model]))\n",
    "    print(model + ': AUC=' + '{0:.3e}'.format(auc_archs[model]))\n",
    "    print('')\n",
    "plt.ylabel('$1/\\epsilon_B$')\n",
    "plt.xlabel('$\\epsilon_S$')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 1)\n",
    "plt.legend(loc=(1.05,0.3), frameon=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again makes it look like training longer is not helping, but this is coming from our over fitting of the data. It is time to reugularize our network!\n",
    "\n",
    "## Regularize the network\n",
    "One way of regularizing is to add a term to the loss function which is propotional to the each parameter (or weight), or the parameter squared. Then the gradient of the whole loss function wants to drive the weights to smaller values. It isn't intuitive, but this helps it to not make large weights just to fit specific events, but it has to work on a more broad set of features.\n",
    "\n",
    "The other method, which is becoming much more popuar, is dropout. In the training step, we randomly remove some of the connections between nodes. This makes some of the pathways between the input and the output stop. Each time we update the network, the dropped out connections are randomly chosen. This again makes it generalize. This adds a hyperparameter to the model (the rate at which we randomly drop connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your favorite model from above again, but now add\n",
    "```\n",
    "tmp_model.add(Dropout(0.5))\n",
    "```\n",
    "after each dense layer (other than the final layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tmp_model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "tmp_model.add(Dense(1, activation='sigmoid'))\n",
    "tmp_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-3))\n",
    "archs['Dropout'] = tmp_model\n",
    "archs['Dropout'].summary()\n",
    "\n",
    "tmp_hist = tmp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_split=0.1,\n",
    "                         epochs=100,\n",
    "                         callbacks=my_callbacks,\n",
    "                         verbose=1\n",
    "                        )\n",
    "\n",
    "training_histories['Dropout'] = tmp_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that our val_loss is much closer to the loss for a long time. Let's plot this compared to the EarlyStopping model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(training_histories['EarlyStopping'].history['loss'], color='C0', label='Final')\n",
    "plt.plot(training_histories['EarlyStopping'].history['val_loss'], color='C0', ls=':')\n",
    "plt.plot(training_histories['Dropout'].history['loss'], color='C1', label='Dropout')\n",
    "plt.plot(training_histories['Dropout'].history['val_loss'], color='C1', ls=':')\n",
    "\n",
    "plt.legend(loc=(1.05, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How do the losses compare when we use dropout?\n",
    " \n",
    " **Optional Exercise:** Go back and change the dropout rates to see what their effects on the results.\n",
    " \n",
    " Finally, compute the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "preds_archs['Dropout'] = tmp_model.predict(X_test_scaled)\n",
    "fpr_archs['Dropout'], tpr_archs['Dropout'], thresholds_archs['Dropout'] = roc_curve(y_true=y_test,\n",
    "                                                                        y_score=preds_archs['Dropout'])\n",
    "auc_archs['Dropout'] = auc(fpr_archs['Dropout'], tpr_archs['Dropout'])\n",
    "\n",
    "background_eff = interp1d(tpr_archs['Dropout'], fpr_archs['Dropout'])\n",
    "working_point_archs['Dropout'] = background_eff(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "my_compare.append('Dropout')\n",
    "for model in my_compare:\n",
    "    plt.plot(tpr_archs[model], 1/fpr_archs[model], label=model)\n",
    "    print(model + ':1/e_B at a fixed e_S=0.5' + ': {0:.2f}'.format(1/working_point_archs[model]))\n",
    "    print(model + ': AUC=' + '{0:.3e}'.format(auc_archs[model]))\n",
    "\n",
    "plt.ylabel('$1/\\epsilon_B$')\n",
    "plt.xlabel('$\\epsilon_S$')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 1)\n",
    "plt.legend(loc=(1.05,0.3), frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model generalizes better than before, but the results have not really improved. We seem to be reaching the limit for the data we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different representation of the data\n",
    "\n",
    "Now we can check to see we can get the same performance with a smarter representation of the data. We will use the basis from [How much information is in a jet](https://arxiv.org/abs/1704.08249). We will use [pyjet](https://github.com/scikit-hep/pyjet) which is built on top of [fastjet](http://fastjet.fr/) to recluster our jets. Using the exclusive jets feature will allow us to pick the axis to compare against.\n",
    "\n",
    "The next section does not have many comments, but will be updated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyjet import cluster,DTYPE_PTEPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the n-subjettiness, we will need to compute the $\\Delta R = \\sqrt{\\Delta\\phi^2 + \\Delta \\eta^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def DeltaPhi(obj_1, obj_2):\n",
    "    '''\n",
    "    Calculates the delta phi, with values between -pi and pi\n",
    "    inputs should be a list with a length of 3\n",
    "    \n",
    "    arguments:\n",
    "    obj_1 = [pt, eta, phi]\n",
    "    obj_2 = [pt, eta, phi]\n",
    "    \n",
    "    returns:\n",
    "    deltaphi -- float for the angular distance\n",
    "    '''\n",
    "    pt1, eta1, phi1 = obj_1\n",
    "    pt2, eta2, phi2 = obj_2\n",
    "    deltaphi = phi1 - phi2\n",
    "    if deltaphi > np.pi:\n",
    "        deltaphi = (-2 * np.pi) + deltaphi\n",
    "    return deltaphi\n",
    "\n",
    "\n",
    "def DeltaR(obj_1, obj_2):\n",
    "    '''\n",
    "    Calculates the delta R\n",
    "    inputs should be a list with a length of 3\n",
    "    obj_1 = [pt, eta, phi]\n",
    "    obj_2 = [pt, eta, phi]\n",
    "    '''\n",
    "    pt1, eta1, phi1 = obj_1\n",
    "    pt2, eta2, phi2 = obj_2\n",
    "    dr = np.sqrt(np.square(eta1 - eta2) + np.square(DeltaPhi(obj_1, obj_2)))\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def NSubjettiness(constituents, axes, beta, jetpt):\n",
    "    '''\n",
    "    Computes the n-subjettines\n",
    "    \n",
    "    Arguments:\n",
    "    constituents -- an array of pseudovectors\n",
    "    axes -- an array of pseudovectors\n",
    "    jetpt -- the overall transverse momentum of the jet\n",
    "        \n",
    "    Returns:\n",
    "    tau[0.5] -- n-subjets with the power of 0.5\n",
    "    tau[1] -- n-subjets with the power of 1\n",
    "    tau[2] -- n-subjets with the power of 2\n",
    "    '''\n",
    "    tau = {0.5: 0.0,\n",
    "           1: 0.0,\n",
    "           2: 0.0\n",
    "           }  # starting values for different betas\n",
    "    for particle in constituents:\n",
    "        for beta in [0.5, 1, 2]:\n",
    "            dr = np.min([np.power(DeltaR([particle.pt, particle.eta, particle.phi],\n",
    "                                         [axis.pt, axis.eta, axis.phi]), beta) for axis in axes])\n",
    "            tau[beta] += particle.pt * dr\n",
    "    for beta in [0.5, 1, 2]:\n",
    "        tau[beta] = tau[beta] / jetpt\n",
    "\n",
    "    return tau[0.5], tau[1], tau[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pseudojets_input = np.zeros(len([x for x in X_train[i][::3] if x > 0]), dtype=DTYPE_PTEPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NSubjettinessData = []\n",
    "\n",
    "for jet_consituents in X_train:\n",
    "    pseudojets_input = np.zeros(len([x for x in jet_consituents[::3] if x > 0]), dtype=DTYPE_PTEPM)\n",
    "    \n",
    "    for j in range(pseudojets_input.shape[0]):\n",
    "        if (jet_consituents[j*3] > 0):  # if the pT bigger than 0\n",
    "            pseudojets_input[j]['pT'] = jet_consituents[j*3]\n",
    "            pseudojets_input[j]['eta'] = jet_consituents[j*3+1]\n",
    "            pseudojets_input[j]['phi'] = jet_consituents[j*3+2]\n",
    "\n",
    "    sequence = cluster(pseudojets_input, R=1, p=1)\n",
    "    jet = sequence.exclusive_jets(1)[0]\n",
    "    NSubjet = {}\n",
    "    for pronginess in range(1, 5):\n",
    "        # undo the clustering to a specific number of jets\n",
    "        if len(jet.constituents()) > pronginess:\n",
    "            axes = sequence.exclusive_jets(pronginess)\n",
    "            nsubjettiness = NSubjettiness(jet.constituents(), axes,\n",
    "                                          pronginess, jet.pt\n",
    "                                         )\n",
    "            NSubjet['{0}_0.5'.format(pronginess)] = nsubjettiness[0]\n",
    "            NSubjet['{0}_1'.format(pronginess)] = nsubjettiness[1]\n",
    "            NSubjet['{0}_2'.format(pronginess)] = nsubjettiness[2]\n",
    "        else:\n",
    "            NSubjet['{0}_0.5'.format(pronginess)] = 0\n",
    "            NSubjet['{0}_1'.format(pronginess)] = 0\n",
    "            NSubjet['{0}_2'.format(pronginess)] = 0\n",
    "            \n",
    "    tmp_entry = [jet.pt,\n",
    "                 NSubjet['1_0.5'], NSubjet['1_1'], NSubjet['1_2'],\n",
    "                 NSubjet['2_0.5'], NSubjet['2_1'], NSubjet['2_2'],\n",
    "                 NSubjet['3_0.5'], NSubjet['3_1'], NSubjet['3_2'],\n",
    "                 NSubjet['4_1'], NSubjet['4_2']\n",
    "                ]\n",
    "    NSubjettinessData.append(tmp_entry)\n",
    "NSubjettinessData = np.array(NSubjettinessData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NSubjettinessData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NSubjettinessTestData = []\n",
    "\n",
    "for jet_consituents in X_test:\n",
    "    pseudojets_input = np.zeros(len([x for x in jet_consituents[::3] if x > 0]), dtype=DTYPE_PTEPM)\n",
    "    \n",
    "    for j in range(pseudojets_input.shape[0]):\n",
    "        if (jet_consituents[j*3] > 0):  # if the pT bigger than 0\n",
    "            pseudojets_input[j]['pT'] = jet_consituents[j*3]\n",
    "            pseudojets_input[j]['eta'] = jet_consituents[j*3+1]\n",
    "            pseudojets_input[j]['phi'] = jet_consituents[j*3+2]\n",
    "\n",
    "    sequence = cluster(pseudojets_input, R=1, p=1)\n",
    "    jet = sequence.exclusive_jets(1)[0]\n",
    "    NSubjet = {}\n",
    "    for pronginess in range(1, 5):\n",
    "        # undo the clustering to a specific number of jets\n",
    "        if len(jet.constituents()) > pronginess:\n",
    "            axes = sequence.exclusive_jets(pronginess)\n",
    "            nsubjettiness = NSubjettiness(jet.constituents(), axes,\n",
    "                                          pronginess, jet.pt\n",
    "                                         )\n",
    "            NSubjet['{0}_0.5'.format(pronginess)] = nsubjettiness[0]\n",
    "            NSubjet['{0}_1'.format(pronginess)] = nsubjettiness[1]\n",
    "            NSubjet['{0}_2'.format(pronginess)] = nsubjettiness[2]\n",
    "        else:\n",
    "            NSubjet['{0}_0.5'.format(pronginess)] = 0\n",
    "            NSubjet['{0}_1'.format(pronginess)] = 0\n",
    "            NSubjet['{0}_2'.format(pronginess)] = 0\n",
    "            \n",
    "    tmp_entry = [jet.pt,\n",
    "                 NSubjet['1_0.5'], NSubjet['1_1'], NSubjet['1_2'],\n",
    "                 NSubjet['2_0.5'], NSubjet['2_1'], NSubjet['2_2'],\n",
    "                 NSubjet['3_0.5'], NSubjet['3_1'], NSubjet['3_2'],\n",
    "                 NSubjet['4_1'], NSubjet['4_2']\n",
    "                ]\n",
    "    NSubjettinessTestData.append(tmp_entry)\n",
    "NSubjettinessTestData = np.array(NSubjettinessTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NSubjettinessTestData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets rescale the data and fit some networks again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SS_NSubJet = StandardScaler()\n",
    "X_Train_NSubJet = SS.fit_transform(NSubjettinessData)\n",
    "X_Test_NSubJet = SS.transform(NSubjettinessTestData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "Now our input data is only 12 dimensional. Lets try starting with a smaller network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_subjet = Sequential()\n",
    "model_subjet.add(Dense(25, input_dim=12, activation='relu'))\n",
    "model_subjet.add(Dense(25, activation='relu'))\n",
    "model_subjet.add(Dense(25, activation='relu'))\n",
    "model_subjet.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_subjet.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_subjet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hist_model_subjet = model_subjet.fit(X_Train_NSubJet,\n",
    "                                     y_train,\n",
    "                                     validation_split=0.1,\n",
    "                                     epochs=100,\n",
    "                                     verbose=2,\n",
    "                                     callbacks=my_callbacks\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predictions and compute the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "preds_sub = model_subjet.predict(X_Test_NSubJet)\n",
    "fpr_sub, tpr_sub, thresholds_sub = roc_curve(y_true=y_test,\n",
    "                                             y_score=preds_sub)\n",
    "auc_sub = auc(fpr_sub, tpr_sub)\n",
    "background_eff = interp1d(tpr_sub, fpr_sub)\n",
    "working_point_sub = background_eff(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(training_histories['EarlyStopping'].history['loss'], color='C0', label='Final')\n",
    "plt.plot(training_histories['EarlyStopping'].history['val_loss'], color='C0', ls=':')\n",
    "plt.plot(training_histories['Dropout'].history['loss'], color='C1', label='Dropout')\n",
    "plt.plot(training_histories['Dropout'].history['val_loss'], color='C1', ls=':')\n",
    "\n",
    "plt.plot(hist_model_subjet.history['loss'], color='C2', label='Subjets')\n",
    "plt.plot(hist_model_subjet.history['val_loss'], color='C2', ls=':')\n",
    "\n",
    "\n",
    "plt.legend(loc=(1.05, 0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Exercises:**\n",
    " 1. Iterate over more model architectures for the NSubjettiness data\n",
    " 2. Only use 2, 3, 4, body N-subjettiness to find when the results saturate."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "336px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
