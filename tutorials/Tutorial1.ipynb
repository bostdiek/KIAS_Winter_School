{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Intoduction-to-Machine-Learning:-Linear-and-Logistic-Regression\" data-toc-modified-id=\"Intoduction-to-Machine-Learning:-Linear-and-Logistic-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intoduction to Machine Learning: Linear and Logistic Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Packages\" data-toc-modified-id=\"Packages-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Packages</a></div><div class=\"lev1 toc-item\"><a href=\"#Linear-regression:-problem-overview\" data-toc-modified-id=\"Linear-regression:-problem-overview-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Linear regression: problem overview</a></div><div class=\"lev2 toc-item\"><a href=\"#Define-the-loss\" data-toc-modified-id=\"Define-the-loss-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Define the loss</a></div><div class=\"lev2 toc-item\"><a href=\"#Define-the-gradient\" data-toc-modified-id=\"Define-the-gradient-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Define the gradient</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-the-loss-and-gradient-together\" data-toc-modified-id=\"Compute-the-loss-and-gradient-together-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Compute the loss and gradient together</a></div><div class=\"lev2 toc-item\"><a href=\"#Perform-the-optimization\" data-toc-modified-id=\"Perform-the-optimization-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Perform the optimization</a></div><div class=\"lev2 toc-item\"><a href=\"#Results\" data-toc-modified-id=\"Results-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Results</a></div><div class=\"lev2 toc-item\"><a href=\"#Optional-Exercises\" data-toc-modified-id=\"Optional-Exercises-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Optional Exercises</a></div><div class=\"lev1 toc-item\"><a href=\"#Logistic-Regression:-problem-overview\" data-toc-modified-id=\"Logistic-Regression:-problem-overview-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Logistic Regression: problem overview</a></div><div class=\"lev2 toc-item\"><a href=\"#Loading-data\" data-toc-modified-id=\"Loading-data-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Loading data</a></div><div class=\"lev2 toc-item\"><a href=\"#Sigmoid\" data-toc-modified-id=\"Sigmoid-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Sigmoid</a></div><div class=\"lev2 toc-item\"><a href=\"#Loss-function\" data-toc-modified-id=\"Loss-function-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Loss function</a></div><div class=\"lev2 toc-item\"><a href=\"#Gradients\" data-toc-modified-id=\"Gradients-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Gradients</a></div><div class=\"lev2 toc-item\"><a href=\"#Initialize-the-weights-and-check-matrix-multiplication\" data-toc-modified-id=\"Initialize-the-weights-and-check-matrix-multiplication-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Initialize the weights and check matrix multiplication</a></div><div class=\"lev2 toc-item\"><a href=\"#Forward-and-backward-propagation\" data-toc-modified-id=\"Forward-and-backward-propagation-36\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Forward and backward propagation</a></div><div class=\"lev2 toc-item\"><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-37\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Optimization</a></div><div class=\"lev1 toc-item\"><a href=\"#Introduction-to-Neural-Networks\" data-toc-modified-id=\"Introduction-to-Neural-Networks-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Introduction to Neural Networks</a></div><div class=\"lev2 toc-item\"><a href=\"#Initialize-the-parameters\" data-toc-modified-id=\"Initialize-the-parameters-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initialize the parameters</a></div><div class=\"lev2 toc-item\"><a href=\"#The-loop\" data-toc-modified-id=\"The-loop-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>The loop</a></div><div class=\"lev2 toc-item\"><a href=\"#Loss\" data-toc-modified-id=\"Loss-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Loss</a></div><div class=\"lev2 toc-item\"><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Backpropagation</a></div><div class=\"lev2 toc-item\"><a href=\"#Update-parameters\" data-toc-modified-id=\"Update-parameters-45\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Update parameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Put-it-all-together\" data-toc-modified-id=\"Put-it-all-together-46\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Put it all together</a></div><div class=\"lev2 toc-item\"><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-47\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Discussion</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intoduction to Machine Learning: Linear and Logistic Regression\n",
    "\n",
    "Welcome to the first tutorial. We will use Python packages to build up our intuition for how machine learning with neural networks for modern high energy physics. First, to get comfortable with Python packages, we will do some basic linear regression to fit lines and curves. From this, we will change the task into a classification problem, referred to as logistic regression. This is the natural building block of neural networks.\n",
    "\n",
    "**Instructions:**\n",
    "- Use the built in vectorized functions whenever possible. This means you should not use for/while loops unless asked to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "First, lets load in the packages that we will use.\n",
    "- [numpy](https://www.numpy.org/devdocs/user/index.html) is the package for numerical computations with Python (NUMerical PYthon)\n",
    "- [scipy](https://docs.scipy.org/doc/scipy/reference/) contains some of the numerical proceedures, such as minimizations\n",
    "- [matplotlib](https://matplotlib.org/3.1.0/tutorials/index.html) is a package for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# allows for plotting within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The commands make the plots look nice\n",
    "plt.rcParams.update({'font.family': 'cmr10',\n",
    "                     'font.size': 12,\n",
    "                     'font.serif': 'cmr10',\n",
    "                     'axes.unicode_minus': False,\n",
    "                     'axes.labelsize': 12,\n",
    "                     'figure.figsize': (3, 3),\n",
    "                     'figure.dpi': 80,\n",
    "                     'mathtext.fontset': 'cm',\n",
    "                     'mathtext.rm': 'serif',\n",
    "                     'xtick.direction': 'in',\n",
    "                     'ytick.direction': 'in',\n",
    "                     'xtick.top': True,\n",
    "                     'ytick.right': True,\n",
    "                     'axes.formatter.use_mathtext': True\n",
    "                     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression: problem overview\n",
    "Load in the data using np.load(). We want the files `../data/tutorial_1_data/linear_training.npy` and `../data/tutorial_1_data/linear_testing.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xy_training = np.load('../data/tutorial_1_data/linear_training.npy')\n",
    "xy_testing = np.load('../data/tutorial_1_data/linear_testing.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = xy_training[:, 0]\n",
    "y = xy_training[:, 1]\n",
    "x_test = xy_testing[:, 0]\n",
    "y_test = xy_testing[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label='training data')\n",
    "plt.scatter(x_test, y_test, label='testing data')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data that we will try to fit a line to. We will want to see how close we get to the true values of the slope and $y$ intercept that we put in by hand.\n",
    "\n",
    "## Define the loss\n",
    "In order to get a good fit, we first have to define what constitutes a good fit. For a situation where we are trying to predict a number, we often use the Mean Squared Error. This is given by\n",
    "\n",
    "$L^{\\rm{MSE}} = \\frac{1}{N}\\sum_i^N \\big(y_i - f(x_i) \\big)^2$\n",
    "\n",
    "where $y_i$ is the *true* value in the data and $f(x_i)$ is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate the mean squared error\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- np.array of the observed values\n",
    "    y_pred -- np.array of the predicted values\n",
    "        \n",
    "    Returns:\n",
    "    mse -- scalar of the mean squared error\n",
    "    '''\n",
    "    N = y_true.shape[0]\n",
    "    \n",
    "    # compute the MSE\n",
    "    # Do not use a loop\n",
    "    ### Start code here ### (about 1 line of code)\n",
    "    mse = \n",
    "    ### End code here ###\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_true = np.array([1, 2, 3])\n",
    "y_pred = np.array([1.1, 1.9, 3])\n",
    "print('MSE = ' + str(MSE(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>MSE</td>\n",
    "    <td> 0.006666666666666678</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the gradient\n",
    "To find the minimum of the loss function (minimizing the error) we need to have the gradient of the loss with respect to the parameters of the model. As our data looks linear, let's use a simple form of $f(x_i) = m~x_i+b$. We write this out with the chain rule as a precursor for the deep neural networks.\n",
    "- Partial derivative with respect to $m$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial m} L^{\\rm{MSE}} &=& \\frac{1}{N} \\sum_i^N~-2\\big(y_i - f(x_i)\\big) ~\\frac{\\partial f(x_i)}{\\partial m}\\\\\n",
    "&=& \\frac{1}{N} \\sum_i^N~-2\\big(y_i - f(x_i)\\big) ~x_i\n",
    "\\end{eqnarray}\n",
    "- Partial derivative with respect to $b$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial b} L^{\\rm{MSE}} &=& \\frac{1}{N} \\sum_i^N~-2\\big(y_i - f(x_i)\\big) ~\\frac{\\partial f(x_i)}{\\partial b}\\\\\n",
    "&=& \\frac{1}{N} \\sum_i^N~-2\\big(y_i - f(x_i)\\big)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the loss and gradient together\n",
    "This will be called the \"forward propagation\" step when we get to neural networks. We want a function that takes in the\n",
    "- model parameters (m and b)\n",
    "- input data\n",
    "- true y values\n",
    "\n",
    "and computes returns the loss and the gradient (for both m and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def propagate(m, b, X, Y):\n",
    "    '''\n",
    "    Implement the loss function and its gradient\n",
    "    \n",
    "    Arguments:\n",
    "    m -- scalar, slope of the line\n",
    "    b -- scalar, bias\n",
    "    X -- np.array\n",
    "    Y -- true value\n",
    "    \n",
    "    Return:\n",
    "    loss -- Mean squared error\n",
    "    dm -- gradient of the loss with respect to m\n",
    "    db -- gradient of the loss with respect to b\n",
    "    '''\n",
    "    ### Implement your code here ###\n",
    "    y_pred = \n",
    "    loss = \n",
    "    ## End code ###\n",
    "    \n",
    "    ### Find gradient\n",
    "    ### Implement your code here ###\n",
    "    dm = \n",
    "    db = \n",
    "    ## End code ###\n",
    "    \n",
    "    assert(dm.dtype == float)\n",
    "    assert(db.dtype == float)\n",
    "    \n",
    "    # this makes a look-up table for easy use later\n",
    "    grads = {'dm': dm,\n",
    "             'db': db\n",
    "            }\n",
    "    \n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m, b, X, Y = 1.0, 2.5, np.array([1, 2, 3, 4]), np.array([1.2, -2.3, 3.0, 4.5])\n",
    "grads, loss = propagate(m, b, X, Y)\n",
    "print('dm = ' + str(grads['dm']))\n",
    "print('db = ' + str(grads['db']))\n",
    "print('MSE = ' + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>dm</td>\n",
    "    <td>15.7</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db</td>\n",
    "    <td>6.8</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>MSE</td>\n",
    "    <td>15.444999999999999</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the optimization\n",
    "**Exercise:** Write down the optimization function. The goal is to learn $m$ and $b$ by minimizing the loss function. For a parameter $\\theta$, the update rule is $\\theta = \\theta - \\alpha~d\\theta$, where $\\alpha$ is the learning rate and $d\\theta$ is the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize(m, b, X, Y, num_iterations, learning_rate, print_loss=False):\n",
    "    '''\n",
    "    The function finds the optimal values for m and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    m -- scalar, slope of the line\n",
    "    b -- scalar, bias\n",
    "    X -- np.array\n",
    "    Y -- true value\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_loss -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the slope m and bias b\n",
    "    grads -- dictionary containing the gradients of the slope and bias with respect to the loss function\n",
    "    losss -- list of all the losss computed during the optimization, this will be used to plot the learning curve.\n",
    "    '''\n",
    "    \n",
    "    losss = []\n",
    "    slopes = []\n",
    "    biases = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        slopes.append(m)\n",
    "        biases.append(b)\n",
    "        \n",
    "        # loss and caculate gradient\n",
    "        ### Start code here ### (about 1 line of code)\n",
    "        grads, loss = \n",
    "        ### End code here ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        ### Start code here ### (about 2 lines of code)\n",
    "        dm = \n",
    "        db =\n",
    "        ### End code here ###\n",
    "        \n",
    "        # update rule\n",
    "        ### Start code here ### (about 2 lines of code)\n",
    "        m = \n",
    "        b = \n",
    "        ### End code here ###\n",
    "        \n",
    "        # Record the losss\n",
    "        losss.append(loss)\n",
    "        \n",
    "        # Print the loss every 100 training iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print('loss after iteration %i: %f' %(i, loss))\n",
    "            \n",
    "    params = {'m': m,\n",
    "              'b': b\n",
    "             }\n",
    "    grads = {'dm': dm,\n",
    "             'db': db\n",
    "            }\n",
    "    \n",
    "    return params, grads, losss, slopes, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now use the `optimize` function to find the best fit line, using an initial guess for $m,b=0$. For now, use a learning rate of 0.01 and at least 1000 iterations of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params, grads, losss, slopes, biases = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results to see if the training worked. The left panel should show the MSE loss, which should decrease with every training step. The next two panels show the value of the slope and $y$-intercept for each training step. These should be asymptoting to values around -2 and 5, respectively. If you do not see this happening, you should debug the `optimize` code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(losss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(slopes)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Slope')\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(biases)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Bias')\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Now plot the final predictions compared to the test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xrange = np.linspace(0, 10)\n",
    "y_pred = xrange * params['m'] + params['b']\n",
    "plt.plot(xrange, y_pred)\n",
    "plt.scatter(x_test, y_test, color='C1', label='testing data')\n",
    "plt.text(7., 1, r'$y=$' + '{0:.3f}'.format(params['m']) + r'$*x + $' + '{0:.3f}'.format(params['b']),\n",
    "         ha='center', va='top')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check the loss for the training and testing values, to make sure that they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Loss_train = MSE(y_true=y, y_pred=x*params['m'] + params['b'])\n",
    "Loss_test = MSE(y_true=y_test, y_pred=x_test*params['m'] + params['b'])\n",
    "print('The losses are: training set={0:.3e}, test set={1:.3e}'.format(Loss_train, Loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the loss is smaller for the training set. This is because our model is trained to minimize the loss on that data. However, the losses are still similar, so it does not look like we have over-fit the data.\n",
    "\n",
    "## Optional Exercises\n",
    "1. What is the effect of the learning rate? Try varying the learning rate between $10^{-3}$ to $1$. What happens when the learning rate is large? Why?\n",
    "2. How would the procedure change if there was a shape to the data? Load the files `../data/tutorial_1_data/linear_regression_curved_training.npy` and `../data/tutorial_1_data/linear_regression_curved_test.npy` and find a function to fit the curve.\n",
    "\n",
    "As an extra thought experiment, this example and the curved example only have one piece of input information (namely $x$). How would we generalize the setup to more than 1 input variabeles? So now, each 'event' $x_i$ is actually a vector which has $j$ dimensions. I will now change notation and write the whole dataset as $X^i_j$, where the lower components represent the vector of information, and the upper component is the different 'events' or datapoints. We will develop this more in the next section, when we explore logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: problem overview\n",
    "In linear regression, we are trying to predict a continuous value. In the literature, it is still referred to as linear regression when the models are more complicated, involving higher powers than just linear. Logisitic regression is used when we want to classify data as belonging to one group or another. In high energy physics, this could come in the form of: was this event new physics or not? was this jet from QCD or a boosted Higgs? We will build up to working with a dataset from our parton showers to ask if the initiating parton of jet was a quark or a gluon.\n",
    "\n",
    "Before we get to the physics cases with many inputs, we will work another simple example using a 2D dataset so that we can easily visualize how things are working. The datasets we will be working with for this example are:\n",
    "- `../data/tutorial_1_data/logistic_regression_training.npy`\n",
    "- `../data/tutorial_1_data/logistic_regression_validation.npy`\n",
    "- `../data/tutorial_1_data/logistic_regression_testing.npy`\n",
    "\n",
    "We have specifically added in a validation set, which will be used to check if the models we make are being overfit to the training set. It is important that the *test* data is not used in optimizing model selections.\n",
    "\n",
    "## Loading data\n",
    "Load the datasets. These numpy arrays have a shape of (number of events, 3). The first two columns of the 3 are $x_1$ and $x_2$. The last columns is a number 0 or 1, which indicates if the event was a background or signal, respectively.\n",
    "\n",
    "**Exercise:** Load the data and seperate out the data columns from the label. (Hint: use the slicing feature of numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training = np.load(...)\n",
    "X_Train = training[:, :2]\n",
    "Y_Train = training[:, 2].reshape(-1, 1)  # makes it a vector\n",
    "\n",
    "validation = np.load(...)\n",
    "X_Val = validation[:, :2]\n",
    "Y_Val = validation[:, 2].reshape(-1, 1)  # makes it a vector\n",
    "\n",
    "testing = np.load(...)\n",
    "X_Test = testing[:, :2]\n",
    "Y_Test = testing[:, 2].reshape(-1, 1)  # makes it a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data to see what we are tryingto fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_Train[np.ravel(Y_Train==1), 0], X_Train[np.ravel(Y_Train==1), 1], color='C0', s=9, label='Sig')\n",
    "plt.scatter(X_Train[np.ravel(Y_Train==0), 0], X_Train[np.ravel(Y_Train==0), 1], color='C3', s=9, label='Back')\n",
    "plt.xlim(-0.05, 5.05)\n",
    "plt.ylim(-0.05, 5.05)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend(loc=(1.05, 0.45), frameon=False, fontsize=14)\n",
    "plt.title('Training Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to find a model which will predict whether a point $(x_1, x_2)$ in parameter space comes from either the Signal set or the Background set. As a first approximation, the boundary between the two is approximately linear. We will use this our first model.\n",
    "\n",
    "## Sigmoid\n",
    "In order to interpret our results, we will first need to modify our model such that the output is always between 0 and 1. Such a function is usually called a sigmoid function. We will use the most common definition as\n",
    "\\begin{equation}\n",
    "\\sigma(z) \\equiv \\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "**Exercise:** Program a function for the sigmoid definition. Then plot $\\sigma(z)$ for $z\\in [-5, 5]$. Hint, use [this](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    \n",
    "    ### Start code here ### (about 1 line of code)\n",
    "    s = \n",
    "    ### End code here ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sigmoid(np.array([0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<table>\n",
    "  <tr>\n",
    "     <td> sigmoid([0,1,2])</td>\n",
    "    <td>array([0.5       , 0.73105858, 0.88079708])</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the function to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 21)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel(r'$\\sigma(z)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple modification, we can now use a very similar model to what we had before. The first part of our linear model will be to take\n",
    "\\begin{equation}\n",
    "z^i = w_1~x_1^i + w_2~x_2^i + b~,\n",
    "\\end{equation}\n",
    "where supsript $i$ refers to the specific data point and the subscript refers to the particular dimension of the data. This should look very similar to our linear model before, where $b$ is again the bias, but now we have two \"slopes\" which we mark as $w_j$ instead of $m$. From now on, these will be called \"weights\". In building up to large dimensional data, we don't want to write out all of this multiplication by hand. Instead we will make a vector $W = (w_1, w_2, \\cdots)$ and use the dot product.\n",
    "\n",
    "In our optimization, we will learn the values of $w_1$, $w_2$, and $b$ which minimizes the loss. Note that we want these to yield **large positive** values of $z$ for events that are signal-like (lower left corner) and **large negative** values for the background-like (upper right). Then, when we feed these through the sigmoid function, they will be mapped to 0 and 1.\n",
    "\n",
    "Feeding the results through the sigmoid is called the *activation*. The sigmoid is not the only function that can be used for this. In the next tutorial we will explore the need for other *Activation Functions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "It is possible to use the mean squared error for classification, however, minimizing this does not lead to the best classification. This can be seen from:\n",
    "1. The largest possible 'error' is 1, so there is no sense of being very wrong in a prediction.\n",
    "2. Large positive or negative values of $z$ asymptote to 0 or 1. With this, the there is little difference in the mean squared error if the model outputs $z=10$ or $z=20$. However, from the underlying definition of $z=W\\cdot X + b$, there is a big difference. This relates to how 'confident' the model is. We want the loss to reward or punish predictions which are confident and right or wrong, respectively.\n",
    "\n",
    "To accomplish this, we use the **Binary Cross Entropy**. For a very nice lecture on how this can be derived from a probability standpoint, see [Lecture 2 of Gilles Louppe's class](https://github.com/glouppe/info8010-deep-learning).\n",
    "\n",
    "The binary cross entropy is defined as\n",
    "\\begin{equation}\n",
    "L^{\\rm{BCE}} = -\\frac{1}{N} \\sum_i^N y^i \\log f(\\mathbf{x}^i) + \\big(1-y^i\\big) \\log\\big(1-f(\\mathbf{x}^i) \\big)\n",
    "\\end{equation}\n",
    "\n",
    "where $y^i$ is again the true label, and $f(\\mathbf{x}^i)$ is the bold and the vector denotes that $\\mathbf{x}$ has multiple dimensions.\n",
    "\n",
    "Let's examine why BCE works and addresses the problems we had with MSE. If the event is from the class $y=1$, then the first term contributes and the second term is identically 0. If the overall prediction is close to 1, the loss asymptotes to 0. So the more confident the model is, the lower the loss. Conversely, the more negative $z$ is, the closer to 0 is the prediction. When we take the $\\log$ of this, the loss blows up (note the overall negative sign in the front of the sum). The story is similar for events coming from $y=0$. Now the first term is identically 0, and the second term contributes. Predictions close to 0 now result in small losses while perdictions close to 1 yield very large losses. While this is not a proof, it at least gives some intuition for why this loss function is used for binary classification problems.\n",
    "\n",
    "**Exercise:** Implement the Binary Cross Entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def BCE(y_true, y_pred):\n",
    "    '''\n",
    "    Computes the binary cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- 1 dimensional numpy array with the true labels. Should all be either 0 or 1\n",
    "    y_pred -- 1 dimensional numpy array containing predictions. Should be continuous between 0 and 1\n",
    "    \n",
    "    Return:\n",
    "    bce -- scalar value\n",
    "    '''\n",
    "    \n",
    "    # Compute the BCE and do not use a loop\n",
    "    ### Start code here ### (about 1 line of code)\n",
    "    bce = \n",
    "    ### End code here ###\n",
    "    \n",
    "    return bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "BCE(np.array([0, 0, 1, 1]), np.array([0.01, 0.99, 0.01, 0.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> BCE([0, 0, 1, 1]), [0.01, 0.99, 0.01, 0.99]) </td>\n",
    "    <td> 2.307610260920796 </td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "We are now almost ready to make our first classiciation model. The last things needed for the optimization are the gradients. You should check the following results. I use $z$ just to help with the chain rule.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial z} L^{\\rm{BCE}}\\big(y_{\\rm{true}}, y_{\\rm{pred}}(z) \\big)\n",
    " & = & \\frac{1}{N}\\sum_i^N \\bigg(\\frac{1-y_{\\rm{true}}}{1-y_{\\rm{pred}}(z)} - \\frac{y_{\\rm{true}}}{y_{\\rm{pred}}(z)} \\bigg) \\frac{\\partial y_{\\rm{pred}}(z)}{\\partial z}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We use $y_{\\rm{pred}}\\big(z(x)\\big) = \\sigma\\big(z(x)\\big)$, \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\sigma\\big(z(x)\\big)}{\\partial x} &=& \\frac{e^{-z(x)}}{\\big(1+e^{-z(x)} \\big)^2} \\frac{\\partial z(x)}{\\partial x} \\\\\n",
    "&=& \\sigma\\big(z(x) \\big) \\Big(1-\\sigma\\big(z(x) \\big) \\Big) \\frac{\\partial z(x)}{\\partial x}\n",
    "\\end{eqnarray}\n",
    "where $x$ is just another dummy variable, and *not* the data.\n",
    "\n",
    "Finally, we use $z^i = W\\cdot \\mathbf{x}^i + b$, giving:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z(w,b)}{\\partial w_j} = x_j^i\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z(w,b)}{\\partial b} = 1\n",
    "\\end{equation}\n",
    "\n",
    "It will be beneficial to program in a function for the derivative of the sigmoid.\n",
    "\n",
    "**Exercise:** Implement the sigmoid_derivative to compute the derivative with respect to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    '''\n",
    "    Compute the gradient of the sigmoid function with respect to its input z.\n",
    "    It can be useful to store this into a variable later\n",
    "    \n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array\n",
    "    \n",
    "    Returns:\n",
    "    ds -- The computed gradient\n",
    "    '''\n",
    "    \n",
    "    ### Start code here ### (around a few lines of code)\n",
    "    s = \n",
    "    ds = \n",
    "    ### End code here ####\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td> sigmoid_derivative([1,2,3])</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights and check matrix multiplication\n",
    "In the linear regression section, we started with the slope and $y$-intercept set to 0. We will want to do the same sort of thing for our weights and bias. We will write this as a function of the number of dimensions so that when we go to larger datasets we don't have to do it by hand\n",
    "\n",
    "**Exercise:** Implement a function to output a vector of zeros for the weights and a single 0 for the bias. Hint: look up [numpy's zeros function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html?highlight=zeros#numpy.zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_zeros(dim):\n",
    "    '''\n",
    "    This function creates a vecor of zeros with shape (dim, 1) for W.\n",
    "    It also inializes b to 0\n",
    "    \n",
    "    Argument:\n",
    "    dim -- interger, the size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar for the bias\n",
    "    '''\n",
    "    \n",
    "    ### Start code here ###\n",
    "    w = \n",
    "    b = \n",
    "    ### end code here\n",
    "    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dim = 2\n",
    "w, b = initialize_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "        <td>  w  </td>\n",
    "        <td> [[ 0.]\n",
    " [ 0.]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  b </td>\n",
    "        <td> 0 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "With this setup, we now have that the shape of $W$ is (2, 1). Each data vector for $X$ is a shape (1, 2) as well. The overall shape of the data is (N, 2). We want to get out the dot product over each row, so the output should be of the shape (N, 1)\n",
    "\n",
    "\\begin{equation}\n",
    "Z = X\\cdot W + b\n",
    "\\end{equation}\n",
    "\n",
    "In some cases (namely doing images) the input data may have yet and extra dimension in the shape (the RGB colors for each pixel). Care needs to be taken in doing the matrix multiplication correctly.\n",
    "\n",
    "**Exercise:** Check that the matrix multiplication works as expected. Note the difference in numpy between `*` and `.dot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use np.dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and backward propagation\n",
    "Now that the parameters have been initalized, we can do the forward and backward steps for learning the parameters.\n",
    "\n",
    "**Exercise:** Implement a function `propagate()` that computes the loss function and gradient.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "Forward propagation:\n",
    "* You get $X$\n",
    "* Compute $A = \\sigma\\big(X\\cdot W +b\\big) = \\big(a_{1}, a_{2},\\cdots,a_{N}\\big)$\n",
    "* Calculate the loss function $L = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log a_i + \\big(1-y_i\\big) \\log\\big(1-a_i\\big)$\n",
    "\n",
    "It will be usefull to fill in the rest of the chain rule and simplify to get\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w^j} = \\frac{1}{N} \\sum_{i=1}^N (a_i - y_i) * x_i^j \n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (a_i - y_i)~.\n",
    "\\end{equation}\n",
    "Again, we want to not use any explicit for loops, so be careful with the shapes when multiplying arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    '''\n",
    "    Implement the loss function and its gradient for the propagation above\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (dim(x[0]), 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data with shape (number of datapoints, dimensionality of the data)\n",
    "    Y -- true label, vector of 0 or 1\n",
    "    \n",
    "    Returns:\n",
    "    loss -- BCE which is the negative log-likelihhood (loss and loss are used interchangeably)\n",
    "    dw -- gradient of the loss with respect to w, needs to be the same shape as w\n",
    "    db -- gradient of the loss with respect to b, needs to be the same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    Write the code step by step and use np.log() and np.dot().\n",
    "    Print shapes if you need to get the dot products correct.\n",
    "    '''\n",
    "    N = X.shape[0]  # number of data points\n",
    "    \n",
    "    # Forward propagation (from X to loss)\n",
    "    # Do not use a loop\n",
    "    ### Start code here ### (around 2 lines of code)\n",
    "    A = \n",
    "    loss = \n",
    "    ### End code here ###\n",
    "    \n",
    "    # Backward propogration (to find the gradient)\n",
    "    ### Start code here ### (around 2 lines of code)\n",
    "    # Do not use a loop\n",
    "    dw = \n",
    "    db = \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    loss = np.squeeze(loss)\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]).T, np.array([[1,0,1]])\n",
    "grads, loss = propagate(w, b, X, Y.reshape(-1, 1))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"loss = \" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  ** dw **  </td>\n",
    "      <td> [[ 0.99845601]\n",
    "     [ 2.39507239]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** db **  </td>\n",
    "        <td> 0.00145557813678 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** loss **  </td>\n",
    "        <td> 5.801545319394553 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Now that we have initialized our parameters and can compute the loss and the gradient, we can find the minimum of the loss function using gradient descent.\n",
    "\n",
    "**Exercise:** Write the optimization function. The goal is to learn $w$ and $b$ minimizing $L^{\\rm{BCE}}$. As before, for a parameter $\\theta$, the update rule is $\\theta = \\theta - \\alpha~ d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_loss=False, X_val=None, Y_val=None):\n",
    "    '''\n",
    "    This function minimizes the BCE loss function by running gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (dim(data), 1)\n",
    "    b -- bias, a scalar\n",
    "    X_train -- data of shape (number of samples, dim(data))\n",
    "    Y_train -- true label of shape (number of samples, 1)\n",
    "    num_iterations -- number of updates using the gradient, an integer\n",
    "    learning_rate -- a float which affects the update size\n",
    "    print_loss -- boolean to print the loss ever 100 steps. Default is False\n",
    "    X_val -- data of shape (number of samples, dim(data)). Default is None\n",
    "    Y_val -- true label of shape (number of samples, 1). Default is None\n",
    "    \n",
    "    If X_val and Y_val are not None, the loss is computed on the validation data\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
    "    losss -- list of all the losss computed during the optimization, this will be used to plot the learning curve.\n",
    "    '''\n",
    "    \n",
    "    losss = {'train': [],\n",
    "             'validation': []\n",
    "            }\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # loss and gradient calculation (around 1-4 lines of code)\n",
    "        ### Start code here ###\n",
    "        grads, loss = \n",
    "        ### end code here\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        ### Start code here ###\n",
    "        dw = \n",
    "        db = \n",
    "        ### End code here ###\n",
    "        \n",
    "        # Calculate the loss on the validation data\n",
    "        if (X_val is not None) and (Y_val is not None):\n",
    "            ### Star code here\n",
    "            # Do not use a loop\n",
    "            A_val = \n",
    "            loss_val = \n",
    "            ### End code here \n",
    "            \n",
    "            losss['validation'].append(loss_val)\n",
    "            \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w =\n",
    "        b =\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the losss\n",
    "        losss['train'].append(loss)\n",
    "        \n",
    "        # Print the loss every 100 training iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"loss after iteration %i: %f\" %(i, loss))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, losss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params, grads, losss = optimize(w, b, X, Y.reshape(-1, 1), num_iterations= 100, learning_rate = 0.009, print_loss = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "    <tr>\n",
    "       <td> w </td>\n",
    "       <td>[[ 0.19033591]\n",
    " [ 0.12259159]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> b </td>\n",
    "       <td> 1.92535983008 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> dw </td>\n",
    "       <td> [[ 0.67752042]\n",
    " [ 1.41625495]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> db </td>\n",
    "       <td> 0.219194504541 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If those worked, we are now ready to classify our example data.\n",
    "\n",
    "**Exercise:** Train a logistic regression model on the example. Use around 10000 iterations and pick an appropriate learning rate. We want the performance to be saturating by the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize w and b\n",
    "w, b = \n",
    "\n",
    "# Optimize\n",
    "params, grads, losss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss as a function of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losss['train'], label='Training')\n",
    "plt.plot(losss['validation'], label='Validation', ls=':')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training and validation losses are nearly identical, so we are not overfitting. This is expected because we are using such an easy model. Now lets plot the decision boundary found by the model. There will be a few lines of matplotlib and numpy in order to plot isocurves of constant model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_space = np.linspace(0, 5, 25)\n",
    "y_space = np.linspace(0, 5, 25)\n",
    "X, Y = np.meshgrid(x_space, y_space)\n",
    "Z = sigmoid(X * params['w'][0] + Y * params['w'][1] + params['b'])  # the 1 - sigmoid just gets the numbering right for the color map below\n",
    "\n",
    "plt.contourf(X, Y, 1-Z, 25, cmap='bwr')\n",
    "CS = plt.contour(X, Y, Z, [0.1, 0.5, 0.9], colors='k')\n",
    "plt.clabel(CS, inline=1, fontsize=12)\n",
    "plt.xlim(-0.05, 5.05)\n",
    "plt.ylim(-0.05, 5.05)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear model draws straight lines through the space. The nominal decision boundard is when the output it $\\ge 0.5$, in which case it is more likely to be signal than background. How does this match up with out test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_Test[Y_Test.ravel()==1, 0], X_Test[Y_Test.ravel()==1, 1], color='b', s=9, alpha=0.5)\n",
    "plt.scatter(X_Test[Y_Test.ravel()==0, 0], X_Test[Y_Test.ravel()==0, 1], color='r', s=9, alpha=0.5)\n",
    "CS = plt.contour(X, Y, Z, [0.1, 0.5, 0.9], colors='k')\n",
    "plt.clabel(CS, inline=1, fontsize=12)\n",
    "plt.xlim(-0.05, 5.05)\n",
    "plt.ylim(-0.05, 5.05)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the data with score to the left of the 0.9 line, we see that most of them are blue, likewise, most of the data to the right of the 0.1 line is red. The areas that the model is confident in do well. However, there are many misclassified points inbetween.\n",
    "\n",
    "**Optional Exercise:**\n",
    "With what we have, we can easily add in more parameters to get a better fit. For instance, we may think that there should be some curvature to the decision boundary. Using the [scikit-learn](https://scikit-learn.org/stable/index.html) package, we can add in all of the quadratic (or higher) terms. Specifically, look up the [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use poly from above\n",
    "X_Quadratic_Train = \n",
    "print(X_Quadratic_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Also do the transormation on the test and validation data\n",
    "X_Quadratic_Test = \n",
    "X_Quadratic_Validatoin = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the model. You can use the same parameters as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize w and b\n",
    "wquad, bquad = \n",
    "\n",
    "# Optimize\n",
    "paramsquad, gradsquad, losssquad = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Z = []\n",
    "for x1 in x_space:\n",
    "    zs = []\n",
    "    for x2 in y_space:\n",
    "        data = poly.transform([[x1, x2]])\n",
    "        zs.append(float(sigmoid(np.dot(data, paramsquad['w']) + paramsquad['b']).flatten()[0]))\n",
    "    Z.append(zs)\n",
    "Z = np.array(Z)\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(X, Y, 1-Z, 25, cmap='bwr')\n",
    "CS = plt.contour(X, Y, Z, [0.1, 0.5, 0.9], colors='k')\n",
    "plt.clabel(CS, inline=1, fontsize=12)\n",
    "plt.xlim(-0.0, 5.0)\n",
    "plt.ylim(-0.0, 5.0)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_Test[Y_Test.ravel()==1, 0], X_Test[Y_Test.ravel()==1, 1], color='b', s=9, alpha=0.5)\n",
    "plt.scatter(X_Test[Y_Test.ravel()==0, 0], X_Test[Y_Test.ravel()==0, 1], color='r', s=9, alpha=0.5)\n",
    "CS = plt.contour(X, Y, Z, [0.1, 0.5, 0.9], colors='k')\n",
    "plt.clabel(CS, inline=1, fontsize=12)\n",
    "plt.xlim(-0.0, 5.0)\n",
    "plt.ylim(-0.0, 5.0)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "\n",
    "plt.tight_layout(w_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "ax0 = plt.subplot(1, 2, 1)\n",
    "plt.plot(losss['train'], label='Training')\n",
    "plt.plot(losss['validation'], label='Validation', ls=':')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.title('Only Linear')\n",
    "\n",
    "plt.subplot(1, 2, 2, sharey=ax0)\n",
    "plt.plot(losssquad['train'], label='Training')\n",
    "plt.plot(losssquad['validation'], label='Validation', ls=':')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.title('All Quadratic')\n",
    "\n",
    "plt.tight_layout(w_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the overall loss lower when the quadratic terms are added, and there is still no overfitting. As a side note, it also trains faster. How do we know when to stop adding in new features? Part of the revolultion in machine learning is that we let the machine choose which features it wants. This will be discussed in the next section, where we build our first neural network to fit the same data.\n",
    "\n",
    "Also note, scikit-learn has [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) already programmed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, out models have gone straight from the input to the output using matrix multiplication and an activation function. By adding an extra layer, we can use the same methodology to make a nerual network. The layer between the input and the output can be viewed as an extra vector, so an initial matrix goes from the inputs to the dimension of the new vector. Then a second matrix goes from the vector to the final answer. This layer is also called a **Hidden Layer**.\n",
    "\n",
    "To add more notation, we will now use superscript brackets to denote the layer number. So the inputs are $X^{i[0]}_j$. The first weight vector, $W^{[1]}$ takes the input to the vector $z^{[1]}$. The explicit equations are:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "z^{i[1]} &=& X^i \\cdot W^{[1]} + b^{[1]} \\\\\n",
    "a^{i[1]} &=& \\sigma\\big(z^{[1]}_{i} \\big) \\\\\n",
    "z^{i[2]} &=& a^{i[1]} \\cdot W^{[2]} + b^{[2]} \\\\\n",
    "y^{i\\rm{,pred}} = a^{i[2]} &=& \\sigma \\big( z^{[2]} \\big)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now we have to minimize the loss over $W^{[1]}, W^{[2]}, b^{[1]}, b^{[2]}$, where the loss is given by\n",
    "\\begin{equation}\n",
    "L^{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\bigg( y^i \\log a^{i[2]} + \\big(1-y^i \\big) \\log\\big(1-a^{i[2]} \\big) \\bigg)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the individual gradients gives\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial z^{i[1]}}{\\partial W^{[1]}} &=& X^i \\\\\n",
    "\\frac{\\partial z^{i[1]}}{\\partial b^{[1]}} &=& 1 \\\\\n",
    "\\frac{\\partial a^{i[1]}}{\\partial z^{i[1]}} &=& a^{i[1]} \\big(1-a^{i[1]} \\big) \\\\\n",
    "& \\rm{and} & \\\\\n",
    "\\frac{\\partial z^{i[2]}}{\\partial W^{[2]}} &=& a^{i[1]} \\\\\n",
    "\\frac{\\partial z^{i[2]}}{\\partial b^{[2]}} &=& 1 \\\\\n",
    "\\frac{\\partial a^{i[2]}}{\\partial z^{i[2]}} &=& a^{i[2]} \\big(1-a^{i[2]} \\big)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these partial derivatives, we can use the chain rule to get\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial W^{[2]}} &=& \\frac{\\partial L}{\\partial a^{[2]}_i}\\frac{\\partial a^{[2]}_i}{\\partial z^{[2]}_i} \\frac{\\partial z^{[2]}_i}{\\partial W^{[2]}} \\\\\n",
    "&=& -\\frac{1}{N} \\sum_{i=1}^N \\bigg(\\frac{y_i}{a^{[2]}_i} - \\frac{1-y_i}{1-a^{[2]}_i} \\bigg) \\bigg(a^{[2]}_i \\big(1-a^{[2]}_i\\big) \\bigg) a^{[1]}_i \\\\\n",
    "&=& -\\frac{1}{N} \\sum_{i=1}^N \\big(y - a^{[2]}_i \\big) a^{[1]}_i\n",
    "\\end{eqnarray}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial b^{[2]}} &=&\n",
    "\\frac{\\partial L}{\\partial a^{[2]}_i}\n",
    "\\frac{\\partial a^{[2]}_i}{\\partial z^{[2]}_i}\n",
    "\\frac{\\partial z^{[2]}_i}{\\partial b^{[2]}} \\\\\n",
    "&=&\n",
    "-\\frac{1}{N} \\sum_{i=1}^N \\bigg(\\frac{y_i}{a^{[2]}_i} - \\frac{1-y_i}{1-a^{[2]}_i} \\bigg) \\bigg(a^{[2]}_i \\big(1-a^{[2]}_i\\big) \\bigg)  \\\\\n",
    "&=& -\\frac{1}{N} \\sum_{i=1}^N \\big(y - a^{[2]}_i \\big)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are similar to what we had for the simple logistic regression. Now we step back to get the gradients for the $^{[1]}$ elements.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial W^{[1]}} &=& \n",
    "\\frac{\\partial L}{\\partial a^{i[2]}}\n",
    "\\frac{\\partial a^{i[2]}}{\\partial z^{i[2]}}\n",
    "\\frac{\\partial z^{i[2]}}{\\partial a^{i[1]}}\n",
    "\\frac{\\partial a^{i[1]}}{\\partial z^{i[1]}}\n",
    "\\frac{\\partial z^{i[1]}}{\\partial W^{[1]}} \\\\\n",
    "%\n",
    "&=& -\\frac{1}{N} \\sum_{i=1}^N \\bigg(\\frac{y^i}{a^{i[2]}} - \\frac{1-y^i}{1-a^{i[2]}} \\bigg) \\bigg(a^{i[2]} \\big(1-a^{i[2]}\\big) \\bigg)\n",
    "\\bigg(W^{[2]} \\bigg)\n",
    "\\bigg(a^{i[1]} \\big(1-a^{i[1]}\\big) \\bigg)\n",
    "X^i \\\\\n",
    "%\n",
    "&=& -\\frac{1}{N} \\sum_{i=1}^N \\bigg(y^i - a^{i[2]} \\bigg)\n",
    "\\bigg(W^{[2]} \\bigg)\n",
    "\\bigg(a^{i[1]} \\big(1-a^{i[1]}\\big) \\bigg)\n",
    "X^i\n",
    "\\end{eqnarray}\n",
    "\n",
    "Similarly,\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial b^{[1]}} &=&\n",
    "\\frac{\\partial L}{\\partial a^{i[2]}}\n",
    "\\frac{\\partial a^{i[2]}}{\\partial z^{i[2]}}\n",
    "\\frac{\\partial z^{i[2]}}{\\partial a^{i[1]}}\n",
    "\\frac{\\partial a^{i[1]}}{\\partial z^{i[1]}}\n",
    "\\frac{\\partial z^{i[1]}}{\\partial b^{[1]}} \\\\\n",
    "%\n",
    "&=& -\\frac{1}{N} \\sum_{i=1}^N \\bigg(y^i - a^{i[2]} \\bigg)\n",
    "\\bigg(W^{[2]} \\bigg)\n",
    "\\bigg(a^{i[1]} \\big(1-a^{i[1]}\\big) \\bigg)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this was more detail than probably necessary, it is seeing the chain of partial derivatives is part of what has lead to the increased performance in machine learning. Keeping track of each partial derivative and the local values for the $a$ make these relatively easy. Note that we will have to be careful with our matrix multiplications, and may need to transpose from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the parameters\n",
    "Now that we know what paramters we want to use, we need to intialize them into memory. Earlier, we set all of the initial parameters to 0, this does not work with hidden layers, so instead we will randomly initialize.\n",
    "- Use `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b)\n",
    "- Our input data will just be the two components.\n",
    "- The biases can still be normalized to 0.\n",
    "\n",
    "**Exercise:** Implement the initialize_random_parameters() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_random_parameters(n_x, n_h):\n",
    "    '''\n",
    "    Arguments:\n",
    "    n_x -- size of the input data\n",
    "    n_h -- size of the hidden layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containg the parameters:\n",
    "        W1 -- weight matrix of shape (n_x, n_h)\n",
    "        b1 -- bias vector of shape (1, n_h)\n",
    "        W2 -- weight matrix of shape (n_h, 1)\n",
    "        b2 -- bias of shape (1, 1) or a scalar\n",
    "    '''\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    ### Start code here ###\n",
    "    W1 = \n",
    "    b1 = \n",
    "    W2 = \n",
    "    b2 = \n",
    "    ### End code here ###\n",
    "    \n",
    "    assert (W1.shape == (n_x, n_h))\n",
    "    assert (b1.shape == (1, n_h))\n",
    "    assert (W2.shape == (n_h, 1))\n",
    "    assert (b2.shape == (1, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "parameters = initialize_random_parameters(2, 4)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "W1 | [[ 0.00496714 -0.00138264  0.00647689  0.0152303 ][-0.00234153 -0.00234137  0.01579213  0.00767435]] |\n",
    "------|------|\n",
    "b1 |  [[0. 0. 0. 0.]]|\n",
    "W2 | [[-0.00469474][ 0.0054256 ][-0.00463418][-0.0046573 ]]|\n",
    "b2 | [[0.]]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loop\n",
    "**Excercise:** Implment the forward_propagation step.\n",
    "\n",
    "**Hints:**\n",
    "* Use the definitions from above\n",
    "* Need to use the paramters from the output of the initialize funciton, i.e. parameter['..']\n",
    "* Values need for the backpropagation are stored in 'cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_samples, n_x)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    # Do not use a loop\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    Z1 = \n",
    "    A1 = \n",
    "    Z2 = \n",
    "    A2 = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (X.shape[0], 1))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_assess = np.array([[0.14092422, 0.80219698],\n",
    "                     [0.07455064, 0.98688694],\n",
    "                     [0.77224477, 0.19871568],\n",
    "                     [0.00552212, 0.81546143],\n",
    "                     [0.70685734, 0.72900717]])\n",
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "# print('A2: ', A2)\n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "[0.005467349042875904 0.5013668146565611 -0.004311049213484426 0.49892223936583624]\n",
    "\n",
    "## Loss\n",
    "Now that we have $a^{[2]}$ for all of the examples, we can compute the loss.\n",
    "\n",
    "**Exercise:**\n",
    "Implement the BCE loss function using the with inputs of `Y, A2, parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_loss(Y, A2, parameters):\n",
    "    '''\n",
    "    Computes the binary cross entroy loss\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- Truth labels vector of size (n_Samples, 1)\n",
    "    A2 -- The sigmoid output of the second activation, shape (n_Samples, 1)\n",
    "    parameters -- python dictionary containing W1, b1, W2, b2\n",
    "    \n",
    "    Returns:\n",
    "    loss -- BCE, scalar\n",
    "    '''\n",
    "    \n",
    "    N = Y.shape[0]  # n_samples\n",
    "    \n",
    "    # Compute the cross entropy\n",
    "    ### Start code here ###\n",
    "    loss =     \n",
    "    ### End code here ###\n",
    "    \n",
    "    loss = np.squeeze(loss)\n",
    "    \n",
    "    assert(isinstance(loss, float))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "Y_assess = np.array([[0], [1], [0], [1], [0]])\n",
    "print(\"loss = \" + str(calculate_loss(Y_assess, A2, parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "loss  | 0.6927166002914666\n",
    "\n",
    "## Backpropagation\n",
    "Once the loss has been calculated, we are ready to do the backprop step, using the saved `cahce` to compute the gradients.\n",
    "\n",
    "**Exercise:** Implement the backward_propagation to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    '''\n",
    "    Computes the gradients\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing W1,b1,W2,b2\n",
    "    cache -- python dictionary containing Z1, A1, Z2, A2\n",
    "    X -- input data of shape (n_samples, n_x)\n",
    "    Y -- true labels of shape (n_samples, 1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing the gradients with respect to W1,b1,W2,b2\n",
    "    '''\n",
    "    \n",
    "    N = X.shape[0]  # n_samples\n",
    "    \n",
    "    # First, pull values out of dictionary parameters\n",
    "    ### Start code here ###\n",
    "    W1 = \n",
    "    W2 = \n",
    "    ### End code here ###\n",
    "    \n",
    "    # Get the values of A1 and A2\n",
    "    ### Start code here ###\n",
    "    A1 = \n",
    "    A2 = \n",
    "    ### End code here ###\n",
    "    \n",
    "    # Compute gradients (hint use chain rule)\n",
    "    ### Start code here \n",
    "    dLdZ2 = \n",
    "    dW2 = \n",
    "    db2 = \n",
    "    dZ1 = \n",
    "    dW1 = \n",
    "    db1 = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(dW2.shape == W2.shape)\n",
    "    assert(dW1.shape == W1.shape)\n",
    "    assert(db1.shape == parameters['b1'].shape)\n",
    "    \n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "parameters = initialize_random_parameters(2, 4)\n",
    "X_assess = np.array([[0.14092422, 0.80219698],\n",
    "                     [0.07455064, 0.98688694],\n",
    "                     [0.77224477, 0.19871568],\n",
    "                     [0.00552212, 0.81546143],\n",
    "                     [0.70685734, 0.72900717]])\n",
    "Y_assess = np.array([[0], [1], [0], [1], [0]])\n",
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "grads = backward_propagation(parameters, cache, X_assess, Y_assess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters\n",
    "Now we need to update the parameters as $\\theta = \\theta - \\alpha \\frac{\\partial L}{\\partial\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.0):\n",
    "    '''\n",
    "    Updates the parameters using theta = theta - lr * dtheta\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing W1,b1,W2,b2\n",
    "    grads -- python dictionary containing dW1,db1,dW2,db2\n",
    "    learning_rate -- float, step size (default is 1.0)\n",
    "    \n",
    "    Outputs:\n",
    "    parameters -- python dictionary containing W1,b1,W2,b2\n",
    "    '''\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    ### Start code here ###\n",
    "    W1 = \n",
    "    b1 = \n",
    "    W2 = \n",
    "    b2 =\n",
    "    ### End code here ###\n",
    "    \n",
    "    # Get gradients\n",
    "    ### Start code here\n",
    "    dW1 =\n",
    "    db1 =\n",
    "    dW2 =\n",
    "    db2 =\n",
    "    ### End code here ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 =\n",
    "    b1 =\n",
    "    W2 =\n",
    "    b2 = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "\n",
    "## Put it all together\n",
    "**Exercise:**\n",
    "Build a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations=10000, validaion_data=None, learning_rate=1e-2, print_loss=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (number of examples, dim)\n",
    "    Y -- labels of shape (number of examples, 1)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    losses = {'train': [],\n",
    "              'validation': []\n",
    "             }\n",
    "    # initialize parameters\n",
    "    n_x = X.shape[1]\n",
    "    parameters = initialize_random_parameters(n_x, n_h)\n",
    "    print(parameters)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        ### Start code here ###\n",
    "        A2, cache = \n",
    "        ### End code here ###\n",
    "        \n",
    "        # Compute loss\n",
    "        ### Start code here ##\n",
    "        loss = \n",
    "        ### End code here ###\n",
    "        losses['train'].append(loss)\n",
    "        \n",
    "        if validaion_data is not None:\n",
    "            x_val = validaion_data[0]\n",
    "            y_val = validaion_data[1]\n",
    "            A2_val, _ = forward_propagation(x_val, parameters)\n",
    "            loss_val = calculate_loss(y_val, A2_val, parameters)\n",
    "            losses['validation'].append(loss_val)\n",
    "        \n",
    "        # Back propagation\n",
    "        ### Start code here ###\n",
    "        grads = \n",
    "        ### End code here ###\n",
    "        \n",
    "        # Update parameters\n",
    "        ### Start code here ###\n",
    "        parameters = \n",
    "        ### End code here ###\n",
    "        \n",
    "        if (print_loss) and (i % 1000 == 0):\n",
    "            print('Loss after {0:04d} of {1:05d} iterations: {2:0.3e}'.format(i,\n",
    "                                                                              num_iterations,\n",
    "                                                                              loss)\n",
    "                 )\n",
    "            \n",
    "    return parameters, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ttrain the neural network. Use between 4 and 10 nodes on the hidden layer and a learnign rate of $10^{-3}$. Train for 100000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nn_params, nn_losses = \n",
    "print(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss as a function of the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(nn_losses['train'])\n",
    "plt.plot(nn_losses['validation'], ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "ax0 = plt.subplot(1, 3, 1)\n",
    "plt.plot(losss['train'], label='Training')\n",
    "plt.plot(losss['validation'], label='Validation', ls=':')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.title('Only Linear')\n",
    "\n",
    "plt.subplot(1, 3, 2, sharey=ax0)\n",
    "plt.plot(losssquad['train'], label='Training')\n",
    "plt.plot(losssquad['validation'], label='Validation', ls=':')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.title('All Quadratic')\n",
    "\n",
    "plt.subplot(1, 3, 3, sharey=ax0)\n",
    "plt.plot(nn_losses['train'], label='Training')\n",
    "plt.plot(nn_losses['validation'], label='Validation', ls=':')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.title('NN (on linear input)')\n",
    "\n",
    "plt.tight_layout(w_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_space = np.linspace(0, 5, 25)\n",
    "y_space = np.linspace(0, 5, 25)\n",
    "X, Y = np.meshgrid(x_space, y_space)\n",
    "Z_nn = []\n",
    "\n",
    "for x in x_space:\n",
    "    z_nn = []\n",
    "    for y in y_space:\n",
    "        A2, _ = forward_propagation(np.array([[x, y]]), nn_params)\n",
    "        z_nn.append(A2.squeeze())\n",
    "    Z_nn.append(z_nn)\n",
    "Z_nn = np.array(Z_nn)\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(X, Y, 1-Z_nn, 25, cmap='bwr')\n",
    "CS = plt.contour(X, Y, Z_nn, [0.1, 0.5, 0.9], colors='k')\n",
    "plt.clabel(CS, inline=1, fontsize=12)\n",
    "plt.xlim(-0.05, 5.05)\n",
    "plt.ylim(-0.05, 5.05)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_Test[Y_Test.ravel()==1, 0], X_Test[Y_Test.ravel()==1, 1], color='b', s=9, alpha=0.5)\n",
    "plt.scatter(X_Test[Y_Test.ravel()==0, 0], X_Test[Y_Test.ravel()==0, 1], color='r', s=9, alpha=0.5)\n",
    "CS = plt.contour(X, Y, Z_nn, [0.1, 0.5, 0.9,], colors='k')\n",
    "plt.clabel(CS, inline=1, fontsize=12)\n",
    "plt.xlim(-0.0, 5.0)\n",
    "plt.ylim(-0.0, 5.0)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "\n",
    "plt.tight_layout(w_pad=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "What happened here? We went through all of that effort and it seems like the neural network is not doing any better than the original logistic regression. It turns out that the choice of the activation function makes a big difference in how well the network performs. We still want to use the sigmoid on the last step to map the answers between 0 and 1. However, for the hidden layer, we can do better if we use a different function. The most common activation function for the middle layers are\n",
    "\n",
    "Name | Equation <img width=200/>| Derivative<img width=200/>\n",
    "--- | --- | --- \n",
    "REctified Linear Unit (ReLU) | $a(z) = \\left\\{\\begin{matrix}0 & z < 0\\\\ z & z >=0\\end{matrix}\\right.$ | $\\frac{\\partial a}{\\partial z} = \\left\\{\\begin{matrix}0 & z < 0\\\\ 1 & z >=0\\end{matrix}\\right.$\n",
    "tanh | $a(z) = \\tanh(z)$ | $\\frac{\\partial a}{\\partial z} = 1-a^2$ |\n",
    "\n",
    "Both of these are simple to implement.\n",
    "\n",
    "**Exercise:**\n",
    "Implement one of the new activation functions.\n",
    "- Define the function (or use np.tanh()) above the `propagate` function cell. Then use this in the a1 = func(z1), but not for the a2 = func(z2).\n",
    "- Add the derivative term to the dZ1 term in the `backward_propagation` cell. \n",
    "- These should be the only changes needed, rerun using the same parameters. You should get out a decision boundary that looks more similar to a curve (but is an interpolation of straight lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Challenge:**\n",
    "\n",
    "The real power of deep learning comes from making the networks deeper, rather than adding more units to the hidden layer. From the code we have here, generalize to network with 2 hidden layers, with the final output now being a3 instead of a2. \n",
    "\n",
    "**Even more challenging:**\n",
    "\n",
    "Adjust the code so that you only need to pick *how many layers* and it will add in the appropriate forward and backward propagation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "444px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
